<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="cv">
<title>Cross-validation of regression models • cv</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Cross-validation of regression models">
<meta property="og:description" content="cv">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">cv</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item">
  <a class="nav-link" href="../articles/cv.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav"></ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="" class="logo" alt=""><h1>Cross-validation of regression models</h1>
                        <h4 data-toc-skip class="author">John Fox and
Georges Monette</h4>
            
            <h4 data-toc-skip class="date">2023-08-11</h4>
      
      
      <div class="d-none name"><code>cv.Rmd</code></div>
    </div>

    
    
<p>Load the packages we’ll use here:</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">cv</span><span class="op">)</span>    <span class="co"># </span></span></code></pre></div>
<div class="section level2">
<h2 id="cross-validation">Cross-validation<a class="anchor" aria-label="anchor" href="#cross-validation"></a>
</h2>
<p>Cross-validation (CV) is an essentially simple and intuitively
reasonable approach to estimating the predictive accuracy of regression
models. CV is developed in many standard sources on regression modeling
and “machine learning”—we particularly recommend <span class="citation">James, Witten, Hastie, &amp; Tibshirani (2021, secs.
5.1, 5.3)</span>—and so we will describe the method only briefly here
before taking up computational issues and some examples.</p>
<p>Validating research by replication on independently collected data is
a common scientific norm. Emulating this process in a single study by
data-division is less common: The data are randomly divided into two,
possibly equal-size, parts; the first part is used to develop and fit a
statistical model; and then the second part is used to assess the
adequacy of the model fit to the first part of the data. Data-division,
however, suffers from two problems: (1) Dividing the data decreases the
sample size and thus increases sampling error; and (2), even more
disconcertingly, particularly in smaller samples, the results can vary
substantially based on the random division of the data: See <span class="citation">Harrell (2015, sec. 5.3)</span> for this and other
remarks about data-division and cross-validation.</p>
<p>Cross-validation speaks to both of these issues. In CV, the data are
randomly divided as equally as possible into several, say <span class="math inline">\(k\)</span>, parts, called “folds.” The statistical
model is fit <span class="math inline">\(k\)</span> times, leaving each
fold out in turn. Each fitted model is then used to predict the response
variable for the omitted fold, computing some CV criterion or “cost”
measure, such as the mean-squared error of prediction. The CV criterion
is then averaged over the <span class="math inline">\(k\)</span> folds.
In the extreme <span class="math inline">\(k = n\)</span>, the number of
cases in the data, thus omitting individual cases and refitting the
model <span class="math inline">\(n\)</span> times—a procedure termed
“leave-one-out (LOO) cross-validation.”</p>
<p>Because the <span class="math inline">\(k\)</span> models are each
fit to <span class="math inline">\(n - 1\)</span> cases, LOO CV produces
a nearly unbiased estimate of prediction error. The <span class="math inline">\(n\)</span> regression models are highly
statistical dependent, however, based as they are on nearly the same
data, and so the resulting estimate of prediction error has relatively
large variance. In contrast, estimated prediction error for <span class="math inline">\(k\)</span>-fold CV with <span class="math inline">\(k = 5\)</span> or <span class="math inline">\(10\)</span> (commonly employed choices) are
somewhat biased but have smaller variance. It is also possible to
correct <span class="math inline">\(k\)</span>-fold CV for bias (see
below).</p>
</div>
<div class="section level2">
<h2 id="examples">Examples<a class="anchor" aria-label="anchor" href="#examples"></a>
</h2>
<div class="section level3">
<h3 id="polynomial-regression-for-the-auto-data">Polynomial regression for the <code>Auto</code> data<a class="anchor" aria-label="anchor" href="#polynomial-regression-for-the-auto-data"></a>
</h3>
<p>The data for this example are drawn from the <strong>ISLR2</strong>
package for R, associated with <span class="citation">James et al.
(2021)</span>, and the presentation here is close (though not identical)
to that in the original source <span class="citation">(James et al.,
2021, secs. 5.1, 5.3)</span>, and it demonstrates the use of the
<code><a href="../reference/cv.html">cv()</a></code> function in the <strong>cv</strong> package.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;span class="citation"&gt;James et al. (2021)&lt;/span&gt; use
the &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv.glm()&lt;/a&gt;&lt;/code&gt; function in the &lt;strong&gt;boot&lt;/strong&gt; package
&lt;span class="citation"&gt;(Canty &amp;amp; Ripley, 2022; Davison &amp;amp; Hinkley,
1997)&lt;/span&gt;. Despite its name, &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv.glm()&lt;/a&gt;&lt;/code&gt; is an independent
function and not a method of a &lt;code&gt;&lt;a href="../reference/cv.html"&gt;cv()&lt;/a&gt;&lt;/code&gt; generic function.&lt;/p&gt;'><sup>1</sup></a></p>
<p>The <code>Auto</code> dataset contains information about 392
cars:</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Auto"</span>, package<span class="op">=</span><span class="st">"ISLR2"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt;   mpg cylinders displacement horsepower weight acceleration year origin</span></span>
<span><span class="co">#&gt; 1  18         8          307        130   3504         12.0   70      1</span></span>
<span><span class="co">#&gt; 2  15         8          350        165   3693         11.5   70      1</span></span>
<span><span class="co">#&gt; 3  18         8          318        150   3436         11.0   70      1</span></span>
<span><span class="co">#&gt; 4  16         8          304        150   3433         12.0   70      1</span></span>
<span><span class="co">#&gt; 5  17         8          302        140   3449         10.5   70      1</span></span>
<span><span class="co">#&gt; 6  15         8          429        198   4341         10.0   70      1</span></span>
<span><span class="co">#&gt;                        name</span></span>
<span><span class="co">#&gt; 1 chevrolet chevelle malibu</span></span>
<span><span class="co">#&gt; 2         buick skylark 320</span></span>
<span><span class="co">#&gt; 3        plymouth satellite</span></span>
<span><span class="co">#&gt; 4             amc rebel sst</span></span>
<span><span class="co">#&gt; 5               ford torino</span></span>
<span><span class="co">#&gt; 6          ford galaxie 500</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/dim.html" class="external-link">dim</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 392   9</span></span></code></pre></div>
<p>With the exception of <code>origin</code> (which we’ll address
later), these variables are largely self-explanatory, with the possible
exception of units of measurement: for details see
<code><a href="https://rdrr.io/pkg/ISLR2/man/Auto.html" class="external-link">help("Auto", package="ISLR2")</a></code>.</p>
<p>We’ll focus here on the relationship of <code>mpg</code> (miles per
gallon) to <code>horsepower</code>, as displayed in the following
scatterplot:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/mpg-horsepower-scatterplot-1.png" alt="`mpg` vs `horsepower` for the `Auto` data" width="100%"><p class="caption">
<code>mpg</code> vs <code>horsepower</code> for the <code>Auto</code>
data
</p>
</div>
<p>The relationship between the two variables is monotone, decreasing,
and nonlinear. Following <span class="citation">James et al.
(2021)</span>, we’ll consider approximating the relationship by a
polynomial regression, with the degree of the polynomial <span class="math inline">\(p\)</span> ranging from 1 (a linear regression) to
10.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Although it serves to illustrate the use of CV, a
polynomial is probably not the best choice here. Consider, for example
the scatterplot for log-transformed &lt;code&gt;mpg&lt;/code&gt; and
&lt;code&gt;horsepower&lt;/code&gt;, produced by
&lt;code&gt;plot(mpg ~ horsepower, data=Auto, log="xy")&lt;/code&gt; (execution of
which is left to the reader).&lt;/p&gt;'><sup>2</sup></a>
Polynomial fits for <span class="math inline">\(p = 1\)</span> to <span class="math inline">\(5\)</span> are shown in the following figure:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="va">horsepower</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="va">horsepower</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/with.html" class="external-link">with</a></span><span class="op">(</span><span class="va">Auto</span>, </span>
<span>                   <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">min</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">horsepower</span><span class="op">)</span>, </span>
<span>                       length<span class="op">=</span><span class="fl">1000</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>,<span class="va">p</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span>  <span class="va">mpg</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html" class="external-link">predict</a></span><span class="op">(</span><span class="va">m</span>, newdata<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>horsepower<span class="op">=</span><span class="va">horsepower</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">mpg</span>, col<span class="op">=</span><span class="va">p</span> <span class="op">+</span> <span class="fl">1</span>, lty<span class="op">=</span><span class="va">p</span>, lwd<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span>
<span><span class="op">}</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, legend<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, col<span class="op">=</span><span class="fl">2</span><span class="op">:</span><span class="fl">6</span>, lty<span class="op">=</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, lwd<span class="op">=</span><span class="fl">2</span>,</span>
<span>       title<span class="op">=</span><span class="st">"Degree"</span>, inset<span class="op">=</span><span class="fl">0.02</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/mpg-horsepower-scatterplot-polynomials-1.png" alt="`mpg` vs `horsepower` for the `Auto` data" width="100%"><p class="caption">
<code>mpg</code> vs <code>horsepower</code> for the <code>Auto</code>
data
</p>
</div>
<p>The linear fit is clearly inappropriate; the fits for <span class="math inline">\(p = 2\)</span> (quadratic) through <span class="math inline">\(4\)</span> are very similar; and the fit for <span class="math inline">\(p = 5\)</span> probably over-fits the data by
chasing one or two relatively high <code>mpg</code> values at the
right.</p>
<p>The following graph shows two measures of estimated squared error as
a function of polynomial-regression degree: The mean-squared error
(MSE), defined as <span class="math inline">\(\mathsf{MSE} = \sum (y_i -
\widehat{y}_i)^2/n\)</span>, and the usual unbiased estimated error
variance, defined as <span class="math inline">\(\widehat(\sigma)^2 =
\sum (y_i - \widehat{y}_i)^2/(n - p - 1)\)</span>. The former
necessarily declines with <span class="math inline">\(p\)</span> (or,
more strictly, can’t increase with <span class="math inline">\(p\)</span>), while the latter gets slightly larger
for the largest values of <span class="math inline">\(p\)</span>, with
the “best” value, by a small margin, for <span class="math inline">\(p =
7\)</span>.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">se</span> <span class="op">&lt;-</span> <span class="va">mse</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/numeric.html" class="external-link">numeric</a></span><span class="op">(</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="kw">for</span> <span class="op">(</span><span class="va">p</span> <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="va">m</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="va">p</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span>  <span class="va">mse</span><span class="op">[</span><span class="va">p</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cost-functions.html">mse</a></span><span class="op">(</span><span class="va">Auto</span><span class="op">$</span><span class="va">mpg</span>, <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">)</span></span>
<span>  <span class="va">se</span><span class="op">[</span><span class="va">p</span><span class="op">]</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m</span><span class="op">)</span><span class="op">$</span><span class="va">sigma</span></span>
<span><span class="op">}</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">10</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/range.html" class="external-link">range</a></span><span class="op">(</span><span class="va">mse</span>, <span class="va">se</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, type<span class="op">=</span><span class="st">"n"</span>,</span>
<span>     xlab<span class="op">=</span><span class="st">"Degree of polynomial, p"</span>,</span>
<span>     ylab<span class="op">=</span><span class="st">"Estimated Squared Error"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="va">mse</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">1</span>, col<span class="op">=</span><span class="fl">2</span>, pch<span class="op">=</span><span class="fl">16</span>, type<span class="op">=</span><span class="st">"b"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/lines.html" class="external-link">lines</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="va">se</span><span class="op">^</span><span class="fl">2</span>, lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">2</span>, col<span class="op">=</span><span class="fl">3</span>, pch<span class="op">=</span><span class="fl">17</span>, type<span class="op">=</span><span class="st">"b"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/legend.html" class="external-link">legend</a></span><span class="op">(</span><span class="st">"topright"</span>, inset<span class="op">=</span><span class="fl">0.02</span>,</span>
<span>       legend<span class="op">=</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/expression.html" class="external-link">expression</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/influence.measures.html" class="external-link">hat</a></span><span class="op">(</span><span class="va">sigma</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span>, <span class="st">"MSE"</span><span class="op">)</span>,</span>
<span>       lwd<span class="op">=</span><span class="fl">2</span>, lty<span class="op">=</span><span class="fl">2</span><span class="op">:</span><span class="fl">1</span>, col<span class="op">=</span><span class="fl">3</span><span class="op">:</span><span class="fl">2</span>, pch<span class="op">=</span><span class="fl">17</span><span class="op">:</span><span class="fl">16</span><span class="op">)</span></span></code></pre></div>
<div class="figure" style="text-align: center">
<img src="fig/mpg-horsepower-MSE-se2-1.png" alt="Estimated squared error as a function of polynomial degree, $p$" width="100%"><p class="caption">
Estimated squared error as a function of polynomial degree, <span class="math inline">\(p\)</span>
</p>
</div>
<p>The code for this graph uses the <code><a href="../reference/cost-functions.html">mse()</a></code> function from the
<strong>cv</strong> package to compute the MSE for each fit.</p>
<div class="section level4">
<h4 id="using-cv">Using <code>cv()</code><a class="anchor" aria-label="anchor" href="#using-cv"></a>
</h4>
<p>The generic <code><a href="../reference/cv.html">cv()</a></code> function has an <code>"lm"</code>
method, which by default performs <span class="math inline">\(k =
10\)</span>-fold CV:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.auto</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="fl">2</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; lm(formula = mpg ~ poly(horsepower, 2), data = Auto)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residuals:</span></span>
<span><span class="co">#&gt;     Min      1Q  Median      3Q     Max </span></span>
<span><span class="co">#&gt; -14.714  -2.594  -0.086   2.287  15.896 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span><span class="co">#&gt; (Intercept)            23.446      0.221   106.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)1 -120.138      4.374   -27.5   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; poly(horsepower, 2)2   44.090      4.374    10.1   &lt;2e-16 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Residual standard error: 4.37 on 389 degrees of freedom</span></span>
<span><span class="co">#&gt; Multiple R-squared:  0.688,  Adjusted R-squared:  0.686 </span></span>
<span><span class="co">#&gt; F-statistic:  428 on 2 and 389 DF,  p-value: &lt;2e-16</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span><span class="op">)</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.196</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.185</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<p>The <code>"lm"</code> method by default uses <code><a href="../reference/cost-functions.html">mse()</a></code> as
the CV criterion and the Woodbury matrix identity to update the
regression with each fold deleted without having literally to refit the
model. Computational details are discussed in the final section of this
vignette. The function reports the CV estimate of MSE, a biased-adjusted
estimate of the MSE (the bias adjustment is explained in the final
section), and the MSE is also computed for the original, full-sample
regression. Because the division of the data into 10 folds is random,
<code><a href="../reference/cv.html">cv()</a></code> explicitly (randomly) generates and saves a seed for
R’s pseudo-random number generator, to make the results replicable. The
user can also specify the seed directly via the <code>seed</code>
argument to <code><a href="../reference/cv.html">cv()</a></code>.</p>
<p>To perform LOO CV, we can set the <code>k</code> argument to
<code><a href="../reference/cv.html">cv()</a></code> to the number of cases in the data, here
<code>k=392</code>, or, more conveniently, to <code>k="loo"</code> or
<code>k="n"</code>:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span></code></pre></div>
<p>For LOO CV of a linear model, <code><a href="../reference/cv.html">cv()</a></code> by default uses the
hatvalues from the model fit to the full data for the LOO updates, and
reports only the CV estimate of MSE. Alternative methods are to use the
Woodbury matrix identity or the “naive” approach of literally refitting
the model with each case omitted. All three methods produce exact
results for a linear model (within the precision of floating-point
computations):</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span>, method<span class="op">=</span><span class="st">"naive"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: naive</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: mse</span></span>
<span><span class="co">#&gt; cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 19.248</span></span>
<span><span class="co">#&gt; full-sample criterion = 18.985</span></span></code></pre></div>
<p>The <code>"naive"</code> and <code>"Woodbury"</code> methods also
return the bias-adjusted estimate of MSE and the full-sample MSE, but
bias isn’t an issue for LOO CV.</p>
<p>This is a small regression problem and all three computational
approaches are essentially instantaneous, but it is still of interest to
investigate their relative speed. In this comparison, we include the
<code><a href="../reference/cv.html">cv.glm()</a></code> function from the <strong>boot</strong> package,
which takes the naive approach, and for which we have to fit the linear
model as an equivalent Gaussian GLM. We use the
<code>microbenchmark()</code> function from the package of the same name
for the timings <span class="citation">(Mersmann, 2023)</span>:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.auto.glm</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></span><span class="op">(</span><span class="va">mpg</span> <span class="op">~</span> <span class="fu"><a href="https://rdrr.io/r/stats/poly.html" class="external-link">poly</a></span><span class="op">(</span><span class="va">horsepower</span>, <span class="fl">2</span><span class="op">)</span>, data<span class="op">=</span><span class="va">Auto</span><span class="op">)</span></span>
<span><span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Auto</span>, <span class="va">m.auto.glm</span><span class="op">)</span><span class="op">$</span><span class="va">delta</span></span>
<span><span class="co">#&gt; [1] 19.248 19.248</span></span>
<span></span>
<span><span class="fu">microbenchmark</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html" class="external-link">microbenchmark</a></span><span class="op">(</span></span>
<span>  hatvalues <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span><span class="op">)</span>,</span>
<span>  Woodbury <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span>,</span>
<span>  naive <span class="op">=</span> <span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.auto</span>, k<span class="op">=</span><span class="st">"loo"</span>, method<span class="op">=</span><span class="st">"naive"</span><span class="op">)</span>,</span>
<span>  cv.glm <span class="op">=</span> <span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Auto</span>, <span class="va">m.auto.glm</span><span class="op">)</span>,</span>
<span>  times<span class="op">=</span><span class="fl">10</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; Unit: microseconds</span></span>
<span><span class="co">#&gt;       expr      min       lq     mean   median       uq      max neval cld</span></span>
<span><span class="co">#&gt;  hatvalues    982.4   1170.4   1217.3   1218.8   1267.9   1434.1    10 a  </span></span>
<span><span class="co">#&gt;   Woodbury  10530.8  10624.2  11232.5  10927.0  11052.6  14672.2    10 a  </span></span>
<span><span class="co">#&gt;      naive 221128.8 222305.4 240480.4 223625.3 279113.2 281829.2    10  b </span></span>
<span><span class="co">#&gt;     cv.glm 387227.5 392630.8 405857.8 393456.7 397338.6 460915.6    10   c</span></span></code></pre></div>
<p>On our computer, using the hatvalues is about an order of magnitude
faster than employing Woodbury matrix updates, and more than two orders
of magnitude faster than refitting the model.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Out of impatience, we asked
&lt;code&gt;microbenchmark()&lt;/code&gt; to execute each command only 10 times
rather than the default 100. With the exception of the last columns, the
output is self-explanatory. The last column shows which methods have
average timings that are statistically distinguishable. Because of the
small number of repetitions (i.e., 10), the &lt;code&gt;"hatvalues"&lt;/code&gt; and
&lt;code&gt;"Woodbury"&lt;/code&gt; methods aren’t distinguishable, but the
difference between these methods persists when we perform more
repetitions—we invite the reader to redo this computation with the
default &lt;code&gt;times=100&lt;/code&gt; repetitions.&lt;/p&gt;'><sup>3</sup></a></p>
</div>
</div>
<div class="section level3">
<h3 id="logistic-regression-for-the-mroz-data">Logistic regression for the <code>Mroz</code> data<a class="anchor" aria-label="anchor" href="#logistic-regression-for-the-mroz-data"></a>
</h3>
<p>The <code>Mroz</code> data set from the <strong>carData</strong>
package <span class="citation">(associated with Fox &amp; Weisberg,
2019)</span> have been used by several authors to illustrate binary
logistic regression; see, in particular <span class="citation">Fox &amp;
Weisberg (2019)</span>. The data were originally drawn from the U.S.
Panel Study of Income Dynamics and pertain to married women. Here are a
few cases in the data set:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"Mroz"</span>, package<span class="op">=</span><span class="st">"carData"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;   lfp k5 k618 age wc hc    lwg   inc</span></span>
<span><span class="co">#&gt; 1 yes  1    0  32 no no 1.2102 10.91</span></span>
<span><span class="co">#&gt; 2 yes  0    2  30 no no 0.3285 19.50</span></span>
<span><span class="co">#&gt; 3 yes  1    3  35 no no 1.5141 12.04</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">tail</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="fl">3</span><span class="op">)</span></span>
<span><span class="co">#&gt;     lfp k5 k618 age wc hc     lwg    inc</span></span>
<span><span class="co">#&gt; 751  no  0    0  43 no no 0.88814  9.952</span></span>
<span><span class="co">#&gt; 752  no  0    0  60 no no 1.22497 24.984</span></span>
<span><span class="co">#&gt; 753  no  0    3  39 no no 0.85321 28.363</span></span></code></pre></div>
<p>The response variable in the logistic regression is <code>lfp</code>,
labor-force participation, a factor coded <code>"yes"</code> or
<code>"no"</code>. The remaining variables are predictors:</p>
<ul>
<li>
<code>k5</code>, number of children 5 years old of younger in the
woman’s household;</li>
<li>
<code>k618</code>, number of children between 6 and 18 years
old;</li>
<li>
<code>age</code>, in years;</li>
<li>
<code>wc</code>, wife’s college attendance, <code>"yes"</code> or
<code>"no"</code>;</li>
<li>
<code>hc</code>, husband’s college attendance;</li>
<li>
<code>lwg</code>, the woman’s log wage rate if she is employed, or
her <em>imputed</em> wage rate, if she is not <span class="citation">(a
variable that Fox &amp; Weisberg, 2019 show is problematically
defined)</span>; and</li>
<li>
<code>inc</code>, family income, in $1000s, exclusive of wife’s
income.</li>
</ul>
<p>We use the <code><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm()</a></code> function to fit a binary logistic
regression to the <code>Mroz</code> data:</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">m.mroz</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/glm.html" class="external-link">glm</a></span><span class="op">(</span><span class="va">lfp</span> <span class="op">~</span> <span class="va">.</span>, data<span class="op">=</span><span class="va">Mroz</span>, family<span class="op">=</span><span class="va">binomial</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html" class="external-link">summary</a></span><span class="op">(</span><span class="va">m.mroz</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Call:</span></span>
<span><span class="co">#&gt; glm(formula = lfp ~ ., family = binomial, data = Mroz)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Coefficients:</span></span>
<span><span class="co">#&gt;             Estimate Std. Error z value Pr(&gt;|z|)    </span></span>
<span><span class="co">#&gt; (Intercept)  3.18214    0.64438    4.94  7.9e-07 ***</span></span>
<span><span class="co">#&gt; k5          -1.46291    0.19700   -7.43  1.1e-13 ***</span></span>
<span><span class="co">#&gt; k618        -0.06457    0.06800   -0.95  0.34234    </span></span>
<span><span class="co">#&gt; age         -0.06287    0.01278   -4.92  8.7e-07 ***</span></span>
<span><span class="co">#&gt; wcyes        0.80727    0.22998    3.51  0.00045 ***</span></span>
<span><span class="co">#&gt; hcyes        0.11173    0.20604    0.54  0.58762    </span></span>
<span><span class="co">#&gt; lwg          0.60469    0.15082    4.01  6.1e-05 ***</span></span>
<span><span class="co">#&gt; inc         -0.03445    0.00821   -4.20  2.7e-05 ***</span></span>
<span><span class="co">#&gt; ---</span></span>
<span><span class="co">#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; (Dispersion parameter for binomial family taken to be 1)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;     Null deviance: 1029.75  on 752  degrees of freedom</span></span>
<span><span class="co">#&gt; Residual deviance:  905.27  on 745  degrees of freedom</span></span>
<span><span class="co">#&gt; AIC: 921.3</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Number of Fisher Scoring iterations: 4</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cost-functions.html">BayesRule</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/ifelse.html" class="external-link">ifelse</a></span><span class="op">(</span><span class="va">Mroz</span><span class="op">$</span><span class="va">lfp</span> <span class="op">==</span> <span class="st">"yes"</span>, <span class="fl">1</span>, <span class="fl">0</span><span class="op">)</span>, </span>
<span>          <span class="fu"><a href="https://rdrr.io/r/stats/fitted.values.html" class="external-link">fitted</a></span><span class="op">(</span><span class="va">m.mroz</span>, type<span class="op">=</span><span class="st">"response"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] 0.30677</span></span></code></pre></div>
<p>In addition to the usually summary output for a GLM, we show the
result of applying the <code><a href="../reference/cost-functions.html">BayesRule()</a></code> function from the
<strong>cv</strong> package to predictions derived from the fitted
model. Bayes rule, which predicts a “success” in a binary regression
model when the fitted probability of success [i.e., <span class="math inline">\(\phi = \Pr(y = 1)\)</span>] is <span class="math inline">\(\widehat{\phi} \ge .5\)</span> and a “failure” if
<span class="math inline">\(\widehat{\phi} \lt .5\)</span>.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;&lt;code&gt;&lt;a href="../reference/cost-functions.html"&gt;BayesRule()&lt;/a&gt;&lt;/code&gt; does some error checking;
&lt;code&gt;&lt;a href="../reference/cost-functions.html"&gt;BayesRule2()&lt;/a&gt;&lt;/code&gt; is similar, but omits the error checking, and
so can be faster for large problems.&lt;/p&gt;'><sup>4</sup></a> The first
argument to <code><a href="../reference/cost-functions.html">BayesRule()</a></code> is the binary {0, 1} response, and
the second argument is the predicted probability of success.
<code><a href="../reference/cost-functions.html">BayesRule()</a></code> returns the proportion of predictions that are
<em>in error</em>, as appropriate for a “cost” function.</p>
<p>In this example, the fitted logistic regression incorrectly predicts
31% of the responses; we expect this estimate to be optimistic given
that the model is used to “predict” the data to which it is fit.</p>
<p>The <code>"glm"</code> method for <code><a href="../reference/cv.html">cv()</a></code> is largely
similar to the <code>"lm"</code> method, although the default algorithm,
selected explicitly by <code>method="exact"</code>, refits the model
with each fold removed (and is thus equivalent to
<code>method="naive"</code> for <code>"lm"</code> models). For
generalized linear models, <code>method="Woodbury"</code> or (for LOO
CV) <code>method="hatvalues"</code> provide approximate results (see the
last section of the vignette for details):</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, seed<span class="op">=</span><span class="fl">248</span><span class="op">)</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: exact</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31952</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, seed<span class="op">=</span><span class="fl">248</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; 10-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32404</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.31926</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span></code></pre></div>
<p>To ensure that the two methods use the same 10 folds, we specify the
seed for R’s random-number generator explicitly; here, and as is common
in our experience, the <code>"exact"</code> and <code>"Woodbury"</code>
algorithms produce nearly identical results. The CV estimates of
prediction error are slightly higher than the estimate based on all of
the cases.</p>
<p>Here are results of applying LOO CV to the Mroz model, using both the
exact and the approximate methods:</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: exact</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: Woodbury</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span>
<span><span class="co">#&gt; bias-adjusted cross-validation criterion = 0.3183</span></span>
<span><span class="co">#&gt; full-sample criterion = 0.30677</span></span>
<span></span>
<span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, method<span class="op">=</span><span class="st">"hatvalues"</span><span class="op">)</span></span>
<span><span class="co">#&gt; n-Fold Cross Validation</span></span>
<span><span class="co">#&gt; method: hatvalues</span></span>
<span><span class="co">#&gt; criterion: BayesRule</span></span>
<span><span class="co">#&gt; cross-validation criterion = 0.32005</span></span></code></pre></div>
<p>To the number of decimal digits shown, the three methods produce
identical results for this example.</p>
<p>As for linear models, we report some timings for the various
<code><a href="../reference/cv.html">cv()</a></code> methods of computation in LOO CV as well as for the
<code><a href="../reference/cv.html">cv.glm()</a></code> function from the <strong>boot</strong> package
(which, recall, refits the model with each case removed, and thus is
comparable to <code><a href="../reference/cv.html">cv()</a></code> with <code>method="exact"</code>):</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">microbenchmark</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/microbenchmark/man/microbenchmark.html" class="external-link">microbenchmark</a></span><span class="op">(</span></span>
<span>  hatvalues<span class="op">=</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, method<span class="op">=</span><span class="st">"hatvalues"</span><span class="op">)</span>,</span>
<span>  Woodbury<span class="op">=</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span>, method<span class="op">=</span><span class="st">"Woodbury"</span><span class="op">)</span>,</span>
<span>  exact<span class="op">=</span><span class="fu"><a href="../reference/cv.html">cv</a></span><span class="op">(</span><span class="va">m.mroz</span>, k<span class="op">=</span><span class="st">"loo"</span>, criterion<span class="op">=</span><span class="va">BayesRule</span><span class="op">)</span>,</span>
<span>  cv.glm<span class="op">=</span><span class="fu">boot</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/boot/man/cv.glm.html" class="external-link">cv.glm</a></span><span class="op">(</span><span class="va">Mroz</span>, <span class="va">m.mroz</span>,</span>
<span>               cost<span class="op">=</span><span class="va">BayesRule</span><span class="op">)</span>,</span>
<span>  times<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="co">#&gt; Unit: milliseconds</span></span>
<span><span class="co">#&gt;       expr       min        lq      mean    median        uq       max neval</span></span>
<span><span class="co">#&gt;  hatvalues    1.2922    1.3246    1.3657    1.3566    1.3914    1.4813    10</span></span>
<span><span class="co">#&gt;   Woodbury   39.2150   41.2499   41.7187   41.4697   42.4250   43.8037    10</span></span>
<span><span class="co">#&gt;      exact 1775.8396 1790.0738 1817.0953 1815.3605 1845.4016 1857.4810    10</span></span>
<span><span class="co">#&gt;     cv.glm 2054.0009 2094.6512 2138.3804 2145.9397 2181.9138 2194.4643    10</span></span>
<span><span class="co">#&gt;   cld</span></span>
<span><span class="co">#&gt;  a   </span></span>
<span><span class="co">#&gt;   b  </span></span>
<span><span class="co">#&gt;    c </span></span>
<span><span class="co">#&gt;     d</span></span></code></pre></div>
<p>There is a substantial time penalty associated with exact
computations.</p>
</div>
</div>
<div class="section level2">
<h2 id="cross-validating-model-selection">Cross-validating model selection<a class="anchor" aria-label="anchor" href="#cross-validating-model-selection"></a>
</h2>
<p>As <span class="citation">Hastie, Tibshirani, &amp; Friedman (2009,
sec. 7.10.2: “The Wrong and Right Way to Do Cross-validation”)</span>,
if the whole data are used to select or fine-tune a statistical model,
subsequent cross-validation of the model is intrinsically misleading,
because the model is selected to fit the whole data, including the part
of the data that remains when each fold is removed.</p>
<p>The following example is similar in spirit to one employed by <span class="citation">Hastie et al. (2009)</span>. Suppose that we randomly
generate <span class="math inline">\(n = 1000\)</span> independent
observations for a response variable variable <span class="math inline">\(y \sim N(\mu = 10, \sigma^2 = 0)\)</span>, and
independently sample <span class="math inline">\(1000\)</span>
observations for each of <span class="math inline">\(p = 100\)</span>
“predictors,” <span class="math inline">\(x_1, \ldots, x_{100}\)</span>,
each from <span class="math inline">\(x_j \sim N(0, 1)\)</span>. The
response has nothing to do with the predictors and so the population
linear-regression model <span class="math inline">\(y_i = \alpha +
\beta_1 x_{i1} + \cdots + \beta_{100} x_{i,100} + \varepsilon_i\)</span>
has <span class="math inline">\(\alpha = 10\)</span> and all <span class="math inline">\(\beta_j = 0\)</span>.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span> <span class="co"># for reproducibility</span></span>
<span><span class="va">y</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1000</span>, mean<span class="op">=</span><span class="fl">10</span><span class="op">)</span></span>
<span><span class="va">X</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/matrix.html" class="external-link">matrix</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fl">1000</span><span class="op">*</span><span class="fl">100</span><span class="op">)</span>, <span class="fl">1000</span>, <span class="fl">100</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/colnames.html" class="external-link">colnames</a></span><span class="op">(</span><span class="va">X</span><span class="op">)</span> <span class="op">&lt;-</span> <span class="fl">1</span><span class="op">:</span><span class="fl">100</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">y</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1]  9.4395  9.7698 11.5587 10.0705 10.1293 11.7151</span></span>
<span><span class="va">X</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span>
<span><span class="co">#&gt;             1        2        3        4        5</span></span>
<span><span class="co">#&gt; [1,] -0.99580 -0.51160 -0.15031  0.19655 -0.49417</span></span>
<span><span class="co">#&gt; [2,] -1.03996  0.23694 -0.32776  0.65011  1.12759</span></span>
<span><span class="co">#&gt; [3,] -0.01798 -0.54159 -1.44817  0.67100 -1.14695</span></span>
<span><span class="co">#&gt; [4,] -0.13218  1.21923 -0.69728 -1.28416  1.48102</span></span>
<span><span class="co">#&gt; [5,] -2.54934  0.17414  2.59849 -2.02611  0.91619</span></span></code></pre></div>
</div>
<div class="section level2 unnumbered">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-CantyRipley2022" class="csl-entry">
Canty, A., &amp; Ripley, B. D. (2022). <em>Boot: Bootstrap
<span>R</span> (<span>S</span>-plus) functions</em>.
</div>
<div id="ref-DavisonHinkley1997" class="csl-entry">
Davison, A. C., &amp; Hinkley, D. V. (1997). <em>Bootstrap methods and
their applications</em>. Cambridge: Cambridge University Press.
</div>
<div id="ref-FoxWeisberg:2019" class="csl-entry">
Fox, J., &amp; Weisberg, S. (2019). <em>An <span>R</span> companion to
applied regression</em> (Third edition). Thousand Oaks <span>CA</span>:
Sage.
</div>
<div id="ref-Harrell:2015" class="csl-entry">
Harrell, F., Jr. (2015). <em>Regression modeling strategies</em> (Second
edition). New York: Springer.
</div>
<div id="ref-HastieTibshiraniFriedman:2009" class="csl-entry">
Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The elements
of statistical learning: Data mining, inference, and prediction</em>
(Second edition). New York: Springer. Retrieved from <a href="https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf" class="external-link">https://hastie.su.domains/ElemStatLearn/printings/ESLII_print12_toc.pdf</a>
</div>
<div id="ref-JamesEtAl:2013" class="csl-entry">
James, G., Witten, D., Hastie, T., &amp; Tibshirani, R. (2021). <em>An
introduction to statistical learning with applications in
<span>R</span></em> (Second edition). New York: Springer. Retrieved from
<a href="https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf" class="external-link">https://hastie.su.domains/ISLR2/ISLRv2_corrected_June_2023.pdf</a>
</div>
<div id="ref-Mersmann:2023" class="csl-entry">
Mersmann, O. (2023). <em>Microbenchmark: Accurate timing functions</em>.
Retrieved from <a href="https://CRAN.R-project.org/package=microbenchmark" class="external-link">https://CRAN.R-project.org/package=microbenchmark</a>
</div>
</div>
</div>

  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by John Fox, Georges Monette.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
