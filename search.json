[{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"adding-a-cost-criterion","dir":"Articles","previous_headings":"","what":"Adding a cost criterion","title":"Extending the cv package","text":"cost criterion suitable use cv() cvSelect() take two arguments, y (observed response vector) yhat (vector fitted predicted response values), return numeric index lack fit. cv package supplies two criteria: mse(y, yhat), returns mean-squared prediction error numeric response; BayesRule(y, yhat) (non-error-checking version, BayesRule2(y, yhat)), suitable use binary regression model, y binary response coded 0 “failure” 1 “success”; yhat predicted probability success; proportion incorrectly classified cases returned. illustrate using different prediction cost criterion, ’ll base cost criterion area receiver operating characteristic (“ROC”) curve logistic regression. ROC curve graphical representation classification power binary regression model, area ROC curve (“AOC”), varies 0 1, common summary measure based ROC (see \"Receiver operating characteristic\", 2023). Metrics package (Hamner & Frasco, 2018) includes variety measures useful model selection, including auc() function. convert AUC cost measure taking complement: apply AUCcomp() Mroz logistic regression discussed main cv package vignette, reproduce , using Mroz data frame carData package (Fox & Weisberg, 2019): Cross-validating cost measure straightforward: expected, cross-validated complement AUC somewhat less optimistic criterion computed model fit whole data set.","code":"AUCcomp <- function(y, yhat) 1 - Metrics::auc(y, yhat) data(\"Mroz\", package=\"carData\") m.mroz <- glm(lfp ~ ., data=Mroz, family=binomial) summary(m.mroz) #>  #> Call: #> glm(formula = lfp ~ ., family = binomial, data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  3.18214    0.64438    4.94  7.9e-07 *** #> k5          -1.46291    0.19700   -7.43  1.1e-13 *** #> k618        -0.06457    0.06800   -0.95  0.34234     #> age         -0.06287    0.01278   -4.92  8.7e-07 *** #> wcyes        0.80727    0.22998    3.51  0.00045 *** #> hcyes        0.11173    0.20604    0.54  0.58762     #> lwg          0.60469    0.15082    4.01  6.1e-05 *** #> inc         -0.03445    0.00821   -4.20  2.7e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  905.27  on 745  degrees of freedom #> AIC: 921.3 #>  #> Number of Fisher Scoring iterations: 4  AUCcomp(with(Mroz, as.numeric(lfp == \"yes\")), fitted(m.mroz)) #> [1] 0.26362 library(\"cv\") #> Loading required package: doParallel #> Loading required package: foreach #> Loading required package: iterators #> Loading required package: parallel cv(m.mroz, criterion=AUCcomp, seed=3639) #> R RNG seed set to 3639 #> 10-Fold Cross Validation #> method: exact #> criterion: AUCcomp #> cross-validation criterion = 0.27682 #> bias-adjusted cross-validation criterion = 0.27636 #> full-sample criterion = 0.26362"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"independently-sampled-cases","dir":"Articles","previous_headings":"Adding a model class not covered by the default cv() method","what":"Independently sampled cases","title":"Extending the cv package","text":"Suppose want cross-validate multinomial logistic regression model fit multinom() function nnet package (Venables & Ripley, 2002). borrow example Fox (2016, sec. 14.2.1), data British Election Panel Study vote choice 2001 British election. Data example BEPS data frame carData package: polytomous (multi-category) response variable vote, factor levels \"Conservative\", \"Labour\", \"Liberal Democrat\". predictors vote : age, years; econ.cond.national econ.cond.household, respondent’s ratings state economy, 1 5 scales. Blair, Hague, Kennedy, ratings leaders Labour, Conservative, Liberal Democratic parties, 1 5 scales. Europe, 11-point scale attitude towards European integration, high scores representing “Euro-skepticism.” political.knowledge, knowledge parties’ positions European integration, scores 0 3. gender, \"female\" \"male\". model fit data includes interaction Europe political.knowledge; predictors enter model additively: predictors, including Europe \\(\\times\\) political.knowledge interaction, associated small \\(p\\)-values; Anova() function car package (Fox & Weisberg, 2019). ’s “effect plot”, using effects package (Fox & Weisberg, 2019) visualize Europe \\(\\times\\) political.knowledge interaction “stacked-area” graph:  cross-validate multinomial-logit model need appropriate cost criterion. Neither criteria supplied cv package—mse(), appropriate numeric response, BayesRule(), appropriate binary response—. One possibility adapt Bayes rule polytomous response: predict() method \"multinom\" models called argument type=\"class\" reports Bayes-rule prediction case—, response category highest predicted probability. BayesRuleMulti() function just reports proportion misclassified cases. marginal proportions response categories marginal Bayes-rule prediction, everyone vote Labour, produces error rate \\(1 - 0.47213 = 0.52787\\). multinomial-logit model appears substantially better , performance hold cross-validation? check first whether default cv() method works “---box” \"multinom\" model: default method getResponse() (function supplied cv package—see ?getResponse) fails \"multinom\" object. straightforward solution supply getResponse.multinom() method returns factor response (using get_response() function insight package, Lüdecke, Waggoner, & Makowski, 2019), try : traceback() (shown) reveals problem default method cv() calls \"multinom\" method predict() argument type=\"response\", correct argument type=\"class\". therefore must write “multinom” method cv(), proves simple: , simply call default cv() method type argument properly set. addition supplying correct type argument, method sets default criterion cv.multinom() method BayesRuleMulti. : Prior invoking cv(), called update() trace=FALSE suppress iteration history reported default multinom()—tedious see iteration history fold. cross-validated polytomous Bayes-rule criterion confirms fitted model substantially better marginal Bayes-rule prediction everyone votes Labour.","code":"data(\"BEPS\", package=\"carData\") head(BEPS) #>               vote age economic.cond.national economic.cond.household Blair #> 1 Liberal Democrat  43                      3                       3     4 #> 2           Labour  36                      4                       4     4 #> 3           Labour  35                      4                       4     5 #> 4           Labour  24                      4                       2     2 #> 5           Labour  41                      2                       2     1 #> 6           Labour  47                      3                       4     4 #>   Hague Kennedy Europe political.knowledge gender #> 1     1       4      2                   2 female #> 2     4       4      5                   2   male #> 3     2       3      3                   2   male #> 4     1       3      4                   0 female #> 5     1       4      6                   2   male #> 6     4       2      4                   2   male library(\"nnet\") m.beps <- multinom(vote ~ age + gender + economic.cond.national +                        economic.cond.household + Blair + Hague + Kennedy +                        Europe*political.knowledge, data=BEPS) #> # weights:  36 (22 variable) #> initial  value 1675.383740  #> iter  10 value 1240.047788 #> iter  20 value 1163.199642 #> iter  30 value 1116.519687 #> final  value 1116.519666  #> converged  car::Anova(m.beps) #> Analysis of Deviance Table (Type II tests) #>  #> Response: vote #>                            LR Chisq Df Pr(>Chisq)     #> age                            13.9  2    0.00097 *** #> gender                          0.5  2    0.79726     #> economic.cond.national         30.6  2    2.3e-07 *** #> economic.cond.household         5.7  2    0.05926 .   #> Blair                         135.4  2    < 2e-16 *** #> Hague                         166.8  2    < 2e-16 *** #> Kennedy                        68.9  2    1.1e-15 *** #> Europe                         78.0  2    < 2e-16 *** #> political.knowledge            55.6  2    8.6e-13 *** #> Europe:political.knowledge     50.8  2    9.3e-12 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 plot(effects::Effect(c(\"Europe\", \"political.knowledge\"), m.beps,             xlevels=list(Europe=1:11, political.knowledge=0:3),             fixed.predictors=list(given.values=c(gendermale=0.5))),      lines=list(col=c(\"blue\", \"red\", \"orange\")),      axes=list(x=list(rug=FALSE), y=list(style=\"stacked\"))) head(BEPS$vote) #> [1] Liberal Democrat Labour           Labour           Labour           #> [5] Labour           Labour           #> Levels: Conservative Labour Liberal Democrat yhat <- predict(m.beps, type=\"class\") head(yhat) #> [1] Labour           Labour           Labour           Labour           #> [5] Liberal Democrat Labour           #> Levels: Conservative Labour Liberal Democrat  BayesRuleMulti <- function(y, yhat){   mean(y != yhat) }  BayesRuleMulti(BEPS$vote, yhat) #> [1] 0.31869 xtabs(~ vote, data=BEPS)/nrow(BEPS) #> vote #>     Conservative           Labour Liberal Democrat  #>          0.30295          0.47213          0.22492 cv(m.beps, seed=3465, criterion=BayesRuleMulti) #> Error in getResponse.default(model): non-vector response getResponse.multinom <- function(model, ...) {   insight::get_response(model) }  head(getResponse(m.beps)) #> [1] Liberal Democrat Labour           Labour           Labour           #> [5] Labour           Labour           #> Levels: Conservative Labour Liberal Democrat cv(m.beps, seed=3465, criterion=BayesRuleMulti) #> R RNG seed set to 3465 #> # weights:  36 (22 variable) #> initial  value 1507.296060  #> iter  10 value 1134.575036 #> iter  20 value 1037.413231 #> iter  30 value 1007.705242 #> iter  30 value 1007.705235 #> iter  30 value 1007.705235 #> final  value 1007.705235  #> converged #> Error in match.arg(type): 'arg' should be one of \"class\", \"probs\" cv.multinom <- function (model, data, criterion=BayesRuleMulti, k, reps,                          seed, ...){   NextMethod(type=\"class\", criterion=criterion) } m.beps <- update(m.beps, trace=FALSE) cv(m.beps, seed=3465) #> R RNG seed set to 3465 #> 10-Fold Cross Validation #> cross-validation criterion = 0.32459 #> bias-adjusted cross-validation criterion = 0.32368 #> full-sample criterion = 0.31869"},{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"mixed-effects-models","dir":"Articles","previous_headings":"Adding a model class not covered by the default cv() method","what":"Mixed-effects models","title":"Extending the cv package","text":"Adding cv() method mixed-model class somewhat complicated. provide cvMixed() function facilitate process, see works, consider \"lme\" method cv package: Notice cv.lme() sets call cvMixed(), computational work. arguments cvMixed() familiar: model mixed-model object, class \"lme\". data data set model fit, default extracted get_data() function insight package. criterion CV criterion, defaulting mse() function. k number CV folds, defaulting \"loo\" CV clusters 10 CV cases. reps number times CV process repeated, defaulting 1. seed seed R’s random-number generator, defaulting randomly selected (saved) value. ncores number cores use parallel computation; 1, default, computation isn’t parallelized. clusterVariables character vector names variables defining clusters; missing, CV based cases rather clusters. nested mixed effects supported. remaining two arguments unfamiliar: predict.clusters.args named list arguments passed predict() function obtain predictions full data set model fit subset data cluster-based CV. first two arguments object model. typically necessary tell cvMixed() base predictions fixed effects; case \"lme\" models, done setting level = 0. Similarly, predict.cases.args named list arguments passed predict() case-based CV. Setting level = 1 includes random effects predictions. Finally, additional arguments, absorbed ..., passed update() model refit fold omitted. cvMixed() returns object class \"cv\". Now imagine want support new class mixed-effects models. concrete, illustrate glmmPQL() function MASS package (Venables & Ripley, 2002), fits generalized-linear mixed-effects models penalized quasi-likelihood.1 coincidentally, arguments glmmPQL() similar lme() (additional family argument), former iteratively invokes latter; cv.glmmPQL() resemble cv.lme(). turns , neither default method getResponse() insight::get_data() work \"glmmPQL\" objects. objects include \"data\" element, however, can simply extract element default data argument cv.glmmPQL() method. get response variable complicated: refit fixed part model GLM regression constant right-hand side, extract response ; need response variable, limit number GLM iterations 1 suppress warning messages non-convergence: Writing cv() method straightforward: set argument verbose=FALSE suppress glmmPQL()’s iteration counter cvMixed() calls update(). Let’s apply newly minted method logistic regression random intercept example appears ?glmmPQL: compare result obtained glmer() lme4 package: two sets estimates similar, identical Finally, try cv.glmmPQL() method, cross-validating clusters cases, compare glmer():","code":"cv:::cv.lme #> function (model, data = insight::get_data(model), criterion = mse,  #>     k, reps = 1, seed, ncores = 1, clusterVariables, ...)  #> { #>     cvMixed(model, data = data, criterion = criterion, k = k,  #>         reps = reps, seed = seed, ncores = ncores, clusterVariables = clusterVariables,  #>         predict.clusters.args = list(object = model, newdata = data,  #>             level = 0), predict.cases.args = list(object = model,  #>             newdata = data, level = 1), ...) #> } #> <bytecode: 0x55d96b59a7c8> #> <environment: namespace:cv> getResponse.glmmPQL <- function(model, ...){   f <- formula(model)   f[[3]] <- 1 # regression constant only on RHS   model <- suppressWarnings(glm(f, data=model$data, family=model$family,                                 control=list(maxit=1)))   cv::getResponse(model) } cv.glmmPQL <- function(model, data = model$data, criterion = mse,                      k, reps = 1, seed, ncores = 1, clusterVariables, ...){   cvMixed(     model,     data=data,     criterion=criterion,     k=k,     reps=reps,     seed=seed,     ncores=ncores,     clusterVariables=clusterVariables,     predict.clusters.args=list(object=model,                                newdata=data,                                level=0,                                type=\"response\"),     predict.cases.args=list(object=model,                             newdata=data,                             level=1,                             type=\"response\"),     verbose=FALSE,     ...) } library(\"MASS\") m.pql <- glmmPQL(y ~ trt + I(week > 2), random = ~ 1 | ID,              family = binomial, data = bacteria) #> iteration 1 #> iteration 2 #> iteration 3 #> iteration 4 #> iteration 5 #> iteration 6 summary(m.pql) #> Linear mixed-effects model fit by maximum likelihood #>   Data: bacteria  #>   AIC BIC logLik #>    NA  NA     NA #>  #> Random effects: #>  Formula: ~1 | ID #>         (Intercept) Residual #> StdDev:      1.4106  0.78005 #>  #> Variance function: #>  Structure: fixed weights #>  Formula: ~invwt  #> Fixed effects:  y ~ trt + I(week > 2)  #>                   Value Std.Error  DF t-value p-value #> (Intercept)      3.4120   0.51850 169  6.5805  0.0000 #> trtdrug         -1.2474   0.64406  47 -1.9367  0.0588 #> trtdrug+        -0.7543   0.64540  47 -1.1688  0.2484 #> I(week > 2)TRUE -1.6073   0.35834 169 -4.4853  0.0000 #>  Correlation:  #>                 (Intr) trtdrg trtdr+ #> trtdrug         -0.598               #> trtdrug+        -0.571  0.460        #> I(week > 2)TRUE -0.537  0.047 -0.001 #>  #> Standardized Within-Group Residuals: #>      Min       Q1      Med       Q3      Max  #> -5.19854  0.15723  0.35131  0.49495  1.74488  #>  #> Number of Observations: 220 #> Number of Groups: 50 library(\"lme4\") #> Loading required package: Matrix m.glmer <- glmer(y ~ trt + I(week > 2) + (1 | ID),                family = binomial, data = bacteria) summary(m.glmer) #> Generalized linear mixed model fit by maximum likelihood (Laplace #>   Approximation) [glmerMod] #>  Family: binomial  ( logit ) #> Formula: y ~ trt + I(week > 2) + (1 | ID) #>    Data: bacteria #>  #>      AIC      BIC   logLik deviance df.resid  #>    202.3    219.2    -96.1    192.3      215  #>  #> Scaled residuals:  #>    Min     1Q Median     3Q    Max  #> -4.561  0.136  0.302  0.422  1.128  #>  #> Random effects: #>  Groups Name        Variance Std.Dev. #>  ID     (Intercept) 1.54     1.24     #> Number of obs: 220, groups:  ID, 50 #>  #> Fixed effects: #>                 Estimate Std. Error z value Pr(>|z|)     #> (Intercept)        3.548      0.696    5.10  3.4e-07 *** #> trtdrug           -1.367      0.677   -2.02  0.04352 *   #> trtdrug+          -0.783      0.683   -1.15  0.25193     #> I(week > 2)TRUE   -1.598      0.476   -3.36  0.00078 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Correlation of Fixed Effects: #>             (Intr) trtdrg trtdr+ #> trtdrug     -0.593               #> trtdrug+    -0.537  0.487        #> I(wk>2)TRUE -0.656  0.126  0.064    # comparison of fixed effects: car::compareCoefs(m.pql, m.glmer)  #> Warning in car::compareCoefs(m.pql, m.glmer): models to be compared are of #> different classes #> Calls: #> 1: glmmPQL(fixed = y ~ trt + I(week > 2), random = ~1 | ID, family =  #>   binomial, data = bacteria) #> 2: glmer(formula = y ~ trt + I(week > 2) + (1 | ID), data = bacteria,  #>   family = binomial) #>  #>                 Model 1 Model 2 #> (Intercept)       3.412   3.548 #> SE                0.514   0.696 #>                                 #> trtdrug          -1.247  -1.367 #> SE                0.638   0.677 #>                                 #> trtdrug+         -0.754  -0.783 #> SE                0.640   0.683 #>                                 #> I(week > 2)TRUE  -1.607  -1.598 #> SE                0.355   0.476 #> cv(m.pql, clusterVariables=\"ID\", criterion=BayesRule) #> n-Fold Cross Validation based on 50 {ID} clusters #> cross-validation criterion = 0.196 #> bias-adjusted cross-validation criterion = 0.196 #> full-sample criterion = 0.19545  cv(m.pql, data=bacteria, criterion=BayesRule, seed=1490) #> R RNG seed set to 1490 #> 10-Fold Cross Validation #> cross-validation criterion = 0.20909 #> bias-adjusted cross-validation criterion = 0.20727 #> full-sample criterion = 0.14545 cv(m.glmer, clusterVariables=\"ID\", criterion=BayesRule) #> n-Fold Cross Validation based on 50 {ID} clusters #> cross-validation criterion = 0.196 #> bias-adjusted cross-validation criterion = 0.196 #> full-sample criterion = 0.19545  cv(m.glmer, data=bacteria, criterion=BayesRule, seed=1490) #> R RNG seed set to 1490 #> 10-Fold Cross Validation #> cross-validation criterion = 0.19545 #> bias-adjusted cross-validation criterion = 0.19364 #> full-sample criterion = 0.15"},{"path":"https://gmonette.github.io/cv/articles/cv-extend.html","id":"adding-a-model-selection-procedure","dir":"Articles","previous_headings":"","what":"Adding a model-selection procedure","title":"Extending the cv package","text":"selectStepAIC() function supplied cv package, based stepAIC() function nnet package (Venables & Ripley, 2002) stepwise model selection, suitable procedure argument cvSelect(). use selectStepAIC() illustrated principal vignette package. ’ll employ selectStepAIC() “template” writing CV model-selection procedure. see code function, type cv::selectStepAIC R command prompt, examine sources cv package https://github.com/gmonette/cv (code selectStepAIC() https://github.com/gmonette/cv/blob/main/R/cvSelect.R). Another approach model selection -subsets regression. regsubsets() function leaps package (Lumley & Miller, 2020) implements efficient algorithm selecting best-fitting linear least-squares regressions subsets predictors sizes, 1 maximum number candidate predictors.2 illustrate use regsubsets(), employ swiss data frame supplied leaps package: data set includes following variables, 47 French-speaking Swiss provinces circa 1888: Fertility: standardized fertility measure. Agriculture: percentage male population engaged agriculture. Examination: percentage draftees Swiss army receiving highest grade examination. Education: percentage draftees primary-school education. Catholic: percentage population Catholic. Infant.Mortality: infant-mortality rate, expressed percentage live births surviving less year. Following Lumley & Miller (2020), treat Fertility response variables predictors linear least-squares regression: Thus, MSE model fit complete data considerably smaller CV estimate MSE. Can better selecting subset predictors, taking account additional uncertainty induced model selection? First, let’s apply best-subset selection complete data set: Selecting best model size. graph, produced subsets() function car package, shows model smallest BIC best model 4 predictors, including Agriculture, Education, Catholic, Infant.Mortality, Examination: MSE selected model (course) slightly higher full model fit previously, cross-validated MSE bit lower; explain main vignette, however, isn’t kosher select cross-validate model data. ’s function named selectSubsets(), meant used cvSelect(), suitable cross-validating model-selection process: slightly tricky point scoping issues, predict() doesn’t work model fit omitting \\(\\)th fold, fitted values cases computed directly \\(\\widehat{\\mathbf{y}}_{-} = \\mathbf{X} \\mathbf{b}_{-}\\), \\(\\mathbf{X}\\) model-matrix cases, \\(\\mathbf{b}_{-}\\) vector least-squares coefficients selected model \\(\\)th fold omitted. Additionally, command lm(y[-indices] ~ X[-indices, x.names.] - 1), selected model \\(\\)th fold deleted, produces awkward coefficient names like \"X[-indices, x.names.]Infant.Mortality\". Purely aesthetic reasons, command sub(\"X\\\\[-indices, x.names.\\\\]\", \"\", names(coefs)) fixes awkward names, removing extraneous text, \"X[-indices, x.names.]\". Applying selectSubsets() full data produces full-data cross-validated MSE (obtained previously): Similarly, applying function imaginary “fold” 5 cases returns MSE cases fold, based model selected fit cases omitting fold; MSE cases, based model; coefficients selected model, includes 4 5 predictors (intercept): , using selectSubsets() cross-validation, get: Cross-validation shows model selection exacts penalty MSE. Examining models selected 10 folds reveals uncertainty identifying predictors “best” model, Agriculture sometimes appearing sometimes :","code":"library(\"leaps\") head(swiss) #>              Fertility Agriculture Examination Education Catholic #> Courtelary        80.2        17.0          15        12     9.96 #> Delemont          83.1        45.1           6         9    84.84 #> Franches-Mnt      92.5        39.7           5         5    93.40 #> Moutier           85.8        36.5          12         7    33.77 #> Neuveville        76.9        43.5          17        15     5.16 #> Porrentruy        76.1        35.3           9         7    90.57 #>              Infant.Mortality #> Courtelary               22.2 #> Delemont                 22.2 #> Franches-Mnt             20.2 #> Moutier                  20.3 #> Neuveville               20.6 #> Porrentruy               26.6 nrow(swiss) #> [1] 47 m.swiss <- lm(Fertility ~ ., data=swiss) summary(m.swiss) #>  #> Call: #> lm(formula = Fertility ~ ., data = swiss) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -15.274  -5.262   0.503   4.120  15.321  #>  #> Coefficients: #>                  Estimate Std. Error t value Pr(>|t|)     #> (Intercept)       66.9152    10.7060    6.25  1.9e-07 *** #> Agriculture       -0.1721     0.0703   -2.45   0.0187 *   #> Examination       -0.2580     0.2539   -1.02   0.3155     #> Education         -0.8709     0.1830   -4.76  2.4e-05 *** #> Catholic           0.1041     0.0353    2.95   0.0052 **  #> Infant.Mortality   1.0770     0.3817    2.82   0.0073 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 7.17 on 41 degrees of freedom #> Multiple R-squared:  0.707,  Adjusted R-squared:  0.671  #> F-statistic: 19.8 on 5 and 41 DF,  p-value: 5.59e-10  cv(m.swiss, seed=8433) #> R RNG seed set to 8433 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 59.683 #> bias-adjusted cross-validation criterion = 58.846 #> full-sample criterion = 44.788 swiss.sub <- regsubsets(Fertility ~ ., data=swiss) summary(swiss.sub) #> Subset selection object #> Call: regsubsets.formula(Fertility ~ ., data = swiss) #> 5 Variables  (and intercept) #>                  Forced in Forced out #> Agriculture          FALSE      FALSE #> Examination          FALSE      FALSE #> Education            FALSE      FALSE #> Catholic             FALSE      FALSE #> Infant.Mortality     FALSE      FALSE #> 1 subsets of each size up to 5 #> Selection Algorithm: exhaustive #>          Agriculture Examination Education Catholic Infant.Mortality #> 1  ( 1 ) \" \"         \" \"         \"*\"       \" \"      \" \"              #> 2  ( 1 ) \" \"         \" \"         \"*\"       \"*\"      \" \"              #> 3  ( 1 ) \" \"         \" \"         \"*\"       \"*\"      \"*\"              #> 4  ( 1 ) \"*\"         \" \"         \"*\"       \"*\"      \"*\"              #> 5  ( 1 ) \"*\"         \"*\"         \"*\"       \"*\"      \"*\"  (bics <- summary(swiss.sub)$bic) #> [1] -19.603 -28.611 -35.656 -37.234 -34.553 which.min(bics) #> [1] 4  car::subsets(swiss.sub, legend=\"topright\") m.best <- update(m.swiss, . ~ . - Examination) summary(m.best) #>  #> Call: #> lm(formula = Fertility ~ Agriculture + Education + Catholic +  #>     Infant.Mortality, data = swiss) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -14.676  -6.052   0.751   3.166  16.142  #>  #> Coefficients: #>                  Estimate Std. Error t value Pr(>|t|)     #> (Intercept)       62.1013     9.6049    6.47  8.5e-08 *** #> Agriculture       -0.1546     0.0682   -2.27   0.0286 *   #> Education         -0.9803     0.1481   -6.62  5.1e-08 *** #> Catholic           0.1247     0.0289    4.31  9.5e-05 *** #> Infant.Mortality   1.0784     0.3819    2.82   0.0072 **  #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 7.17 on 42 degrees of freedom #> Multiple R-squared:  0.699,  Adjusted R-squared:  0.671  #> F-statistic: 24.4 on 4 and 42 DF,  p-value: 1.72e-10  cv(m.best, seed=8433) # use same folds as before #> R RNG seed set to 8433 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 58.467 #> bias-adjusted cross-validation criterion = 57.778 #> full-sample criterion = 45.916 selectSubsets <- function(data=insight::get_data(model),                            model,                           indices,                           criterion=mse,                           save.coef=TRUE, ...){      if (inherits(model, \"lm\", which=TRUE) != 1)     stop(\"selectSubsets is appropriate only for 'lm' models\")      y <- getResponse(model)   formula <- formula(model)   X <- model.matrix(model)    if (missing(indices)) {     # select the best model from the full data by BIC     sel <- leaps::regsubsets(formula, data=data, ...)     bics <- summary(sel)$bic     best <- coef(sel, 1:length(bics))[[which.min(bics)]]     x.names <- names(best)     # fit the best model; intercept is already in X, hence - 1:     m.best <- lm(y ~ X[, x.names] - 1)      fit.all <- predict(m.best, newdata=data)     return(criterion(y, fit.all)) # return the CV criterion   }    # select the best model omitting the i-th fold (given by indices)   sel.i <- leaps::regsubsets(formula, data[-indices, ], ...)   bics.i <- summary(sel.i)$bic   best.i <- coef(sel.i, 1:length(bics.i))[[which.min(bics.i)]]   x.names.i <- names(best.i)   m.best.i <- lm(y[-indices] ~ X[-indices, x.names.i] - 1)               # predict() doesn't work here:   fit.all.i <- as.vector(X[, x.names.i] %*% coef(m.best.i))   fit.i <- fit.all.i[indices]   # return the CV criteria and the regression coefficients   list(criterion=c(criterion(y[indices], fit.i), # for i-th fold                    criterion(y, fit.all.i)), # for all data        coefficients = if (save.coef){          coefs <- coef(m.best.i)                    # fix coefficient names          names(coefs) <- sub(\"X\\\\[-indices, x.names.i\\\\]\", \"\",                              names(coefs))                    coefs        }  else {          NULL        }   ) } selectSubsets(model=m.swiss) #> [1] 45.916 selectSubsets(model=m.swiss, indices=seq(5, 45, by=10)) #> $criterion #> [1] 50.128 46.297 #>  #> $coefficients #>      (Intercept)      Agriculture        Education         Catholic  #>         63.80452         -0.15895         -1.04218          0.13066  #> Infant.Mortality  #>          1.01895 (cv.swiss <- cvSelect(selectSubsets, model=m.swiss,                       data=swiss, seed=8433)) # use same folds #> R RNG seed set to 8433 #> 10-Fold Cross Validation #> cross-validation criterion = 65.835 #> bias-adjusted cross-validation criterion = 63.644 #> full-sample criterion = 45.916 compareFolds(cv.swiss) #>         (Intercept) Catholic Education Infant.Mortality Agriculture #> Fold 1      59.0852   0.1397   -1.0203           1.2985       -0.17 #> Fold 2      67.0335   0.1367   -1.0499           0.9413       -0.20 #> Fold 3      55.0453   0.1221   -0.8757           1.3541       -0.15 #> Fold 4      62.5543   0.1236   -0.9719           1.0679       -0.16 #> Fold 5      50.4643   0.1057   -0.7863           1.2144             #> Fold 6      68.0289   0.1195   -1.0073           0.8294       -0.17 #> Fold 7      66.5219   0.1357   -1.0827           0.9523       -0.19 #> Fold 8      46.3507   0.0776   -0.7637           1.4463             #> Fold 9      62.2632   0.1230   -1.0067           1.1000       -0.17 #> Fold 10     52.5112   0.1005   -0.7232           1.0809"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"cross-validation","dir":"Articles","previous_headings":"","what":"Cross-validation","title":"Cross-validation of regression models","text":"Cross-validation (CV) essentially simple intuitively reasonable approach estimating predictive accuracy regression models. CV developed many standard sources regression modeling “machine learning”—particularly recommend James, Witten, Hastie, & Tibshirani (2021, secs. 5.1, 5.3)—describe method briefly taking computational issues examples. Validating research replication independently collected data common scientific norm. Emulating process single study data-division less common: data randomly divided two, possibly equal-size, parts; first part used develop fit statistical model; second part used assess adequacy model fit first part data. Data-division, however, suffers two problems: (1) Dividing data decreases sample size thus increases sampling error; (2), even disconcertingly, particularly smaller samples, results can vary substantially based random division data: See Harrell (2015, sec. 5.3) remarks data-division cross-validation. Cross-validation speaks issues. CV, data randomly divided equally possible several, say \\(k\\), parts, called “folds.” statistical model fit \\(k\\) times, leaving fold turn. fitted model used predict response variable omitted fold, computing CV criterion “cost” measure, mean-squared error prediction. CV criterion averaged \\(k\\) folds. extreme \\(k = n\\), number cases data, thus omitting individual cases refitting model \\(n\\) times—procedure termed “leave-one-(LOO) cross-validation.” \\(k\\) models fit \\(n - 1\\) cases, LOO CV produces nearly unbiased estimate prediction error. \\(n\\) regression models highly statistical dependent, however, based nearly data, resulting estimate prediction error relatively large variance. contrast, estimated prediction error \\(k\\)-fold CV \\(k = 5\\) \\(10\\) (commonly employed choices) somewhat biased smaller variance. also possible correct \\(k\\)-fold CV bias (see ).","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"polynomial-regression-for-the-auto-data","dir":"Articles","previous_headings":"Examples","what":"Polynomial regression for the Auto data","title":"Cross-validation of regression models","text":"data example drawn ISLR2 package R, associated James et al. (2021), presentation close (though identical) original source (James et al., 2021, secs. 5.1, 5.3), demonstrates use cv() function cv package.1 Auto dataset contains information 392 cars: exception origin (don’t use ), variables largely self-explanatory, except possibly units measurement: details see help(\"Auto\", package=\"ISLR2\"). ’ll focus relationship mpg (miles per gallon) horsepower, displayed following scatterplot: mpg vs horsepower Auto data relationship two variables monotone, decreasing, nonlinear. Following James et al. (2021), ’ll consider approximating relationship polynomial regression, degree polynomial \\(p\\) ranging 1 (linear regression) 10.2 Polynomial fits \\(p = 1\\) \\(5\\) shown following figure: mpg vs horsepower Auto data linear fit clearly inappropriate; fits \\(p = 2\\) (quadratic) \\(4\\) similar; fit \\(p = 5\\) probably -fits data chasing one two relatively high mpg values right. following graph shows two measures estimated squared error function polynomial-regression degree: mean-squared error (MSE), defined \\(\\mathsf{MSE} = \\sum (y_i - \\widehat{y}_i)^2/n\\), usual unbiased estimated error variance, defined \\(\\widehat{\\sigma}^2 = \\sum (y_i - \\widehat{y}_i)^2/(n - p - 1)\\). former necessarily declines \\(p\\) (, strictly, can’t increase \\(p\\)), latter gets slightly larger largest values \\(p\\), “best” value, small margin, \\(p = 7\\). Estimated squared error function polynomial degree, \\(p\\) code graph uses mse() function cv package compute MSE fit.","code":"data(\"Auto\", package=\"ISLR2\") head(Auto) #>   mpg cylinders displacement horsepower weight acceleration year origin #> 1  18         8          307        130   3504         12.0   70      1 #> 2  15         8          350        165   3693         11.5   70      1 #> 3  18         8          318        150   3436         11.0   70      1 #> 4  16         8          304        150   3433         12.0   70      1 #> 5  17         8          302        140   3449         10.5   70      1 #> 6  15         8          429        198   4341         10.0   70      1 #>                        name #> 1 chevrolet chevelle malibu #> 2         buick skylark 320 #> 3        plymouth satellite #> 4             amc rebel sst #> 5               ford torino #> 6          ford galaxie 500 dim(Auto) #> [1] 392   9 plot(mpg ~ horsepower, data=Auto) plot(mpg ~ horsepower, data=Auto) horsepower <- with(Auto,                     seq(min(horsepower), max(horsepower),                         length=1000)) for (p in 1:5){   m <- lm(mpg ~ poly(horsepower,p), data=Auto)   mpg <- predict(m, newdata=data.frame(horsepower=horsepower))   lines(horsepower, mpg, col=p + 1, lty=p, lwd=2) } legend(\"topright\", legend=1:5, col=2:6, lty=1:5, lwd=2,        title=\"Degree\", inset=0.02) library(\"cv\") # for mse() and other functions #> Loading required package: doParallel #> Loading required package: foreach #> Loading required package: iterators #> Loading required package: parallel  se <- mse <- numeric(10) for (p in 1:10){   m <- lm(mpg ~ poly(horsepower, p), data=Auto)   mse[p] <- mse(Auto$mpg, fitted(m))   se[p] <- summary(m)$sigma }  plot(c(1, 10), range(mse, se^2), type=\"n\",      xlab=\"Degree of polynomial, p\",      ylab=\"Estimated Squared Error\") lines(1:10, mse, lwd=2, lty=1, col=2, pch=16, type=\"b\") lines(1:10, se^2, lwd=2, lty=2, col=3, pch=17, type=\"b\") legend(\"topright\", inset=0.02,        legend=c(expression(hat(sigma)^2), \"MSE\"),        lwd=2, lty=2:1, col=3:2, pch=17:16)"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"using-cv","dir":"Articles","previous_headings":"Examples > Polynomial regression for the Auto data","what":"Using cv()","title":"Cross-validation of regression models","text":"generic cv() function \"lm\" method, default performs \\(k = 10\\)-fold CV: \"lm\" method default uses mse() CV criterion Woodbury matrix identity update regression fold deleted without literally refit model. Computational details discussed final section vignette. function reports CV estimate MSE, biased-adjusted estimate MSE (bias adjustment explained final section), MSE also computed original, full-sample regression. division data 10 folds random, cv() explicitly (randomly) generates saves seed R’s pseudo-random number generator, make results replicable. user can also specify seed directly via seed argument cv(). perform LOO CV, can set k argument cv() number cases data, k=392, , conveniently, k=\"loo\" k=\"n\": LOO CV linear model, cv() default uses hatvalues model fit full data LOO updates, reports CV estimate MSE. Alternative methods use Woodbury matrix identity “naive” approach literally refitting model case omitted. three methods produce exact results linear model (within precision floating-point computations): \"naive\" \"Woodbury\" methods also return bias-adjusted estimate MSE full-sample MSE, bias isn’t issue LOO CV. small regression problem three computational approaches essentially instantaneous, still interest investigate relative speed. comparison, include cv.glm() function boot package, takes naive approach, fit linear model equivalent Gaussian GLM. use microbenchmark() function package name timings (Mersmann, 2023): computer, using hatvalues order magnitude faster employing Woodbury matrix updates, two orders magnitude faster refitting model.3","code":"m.auto <- lm(mpg ~ poly(horsepower, 2), data=Auto) summary(m.auto) #>  #> Call: #> lm(formula = mpg ~ poly(horsepower, 2), data = Auto) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -14.714  -2.594  -0.086   2.287  15.896  #>  #> Coefficients: #>                      Estimate Std. Error t value Pr(>|t|)     #> (Intercept)            23.446      0.221   106.1   <2e-16 *** #> poly(horsepower, 2)1 -120.138      4.374   -27.5   <2e-16 *** #> poly(horsepower, 2)2   44.090      4.374    10.1   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 4.37 on 389 degrees of freedom #> Multiple R-squared:  0.688,  Adjusted R-squared:  0.686  #> F-statistic:  428 on 2 and 389 DF,  p-value: <2e-16  cv(m.auto) #> R RNG seed set to 992869 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 19.215 #> bias-adjusted cross-validation criterion = 19.203 #> full-sample criterion = 18.985 cv(m.auto, k=\"loo\") #> n-Fold Cross Validation #> method: hatvalues #> criterion: mse #> cross-validation criterion = 19.248 cv(m.auto, k=\"loo\", method=\"naive\") #> n-Fold Cross Validation #> method: naive #> criterion: mse #> cross-validation criterion = 19.248 #> bias-adjusted cross-validation criterion = 19.248 #> full-sample criterion = 18.985  cv(m.auto, k=\"loo\", method=\"Woodbury\") #> n-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 19.248 #> bias-adjusted cross-validation criterion = 19.248 #> full-sample criterion = 18.985 m.auto.glm <- glm(mpg ~ poly(horsepower, 2), data=Auto) boot::cv.glm(Auto, m.auto.glm)$delta #> [1] 19.248 19.248  microbenchmark::microbenchmark(   hatvalues = cv(m.auto, k=\"loo\"),   Woodbury = cv(m.auto, k=\"loo\", method=\"Woodbury\"),   naive = cv(m.auto, k=\"loo\", method=\"naive\"),   cv.glm = boot::cv.glm(Auto, m.auto.glm),   times=10 ) #> Unit: milliseconds #>       expr       min        lq      mean    median        uq       max neval #>  hatvalues    2.5579    2.6704    2.8881    2.9205    3.0065    3.5324    10 #>   Woodbury   26.1507   26.2335   26.3535   26.2968   26.3919   26.7967    10 #>      naive  596.3805  597.3566  605.4925  599.1753  607.6364  647.7516    10 #>     cv.glm 1033.6599 1038.6703 1072.3685 1048.1588 1055.6901 1203.7609    10"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"comparing-competing-models","dir":"Articles","previous_headings":"Examples > Polynomial regression for the Auto data","what":"Comparing Competing Models","title":"Cross-validation of regression models","text":"cv() function also method can applied list regression models data, composed using models() function. \\(k\\)-fold CV, folds used competing models, reduces random error comparison. result can also obtained specifying common seed R’s random-number generator applying cv() separately model, employing list models convenient \\(k\\)-fold LOO CV (random component composition \\(n\\) folds). illustrate polynomial regression models varying degree Auto data (discussed previously), beginning fitting saving 10 models: apply cv() list 10 models (data argument required): didn’t supply names models calls models() function, names model.1, model.2, etc., supplied function. Finally, extract graph adjusted MSEs \\(10\\)-fold CV MSEs LOO CV: Cross-validated 10-fold LOO MSE function polynomial degree, \\(p\\) Alternatively, can use plot() method \"cvModList\" objects compare models, though separate graphs 10-fold LOO CV: Cross-validated 10-fold LOO MSE function polynomial degree, \\(p\\) example, 10-fold LOO CV produce generally similar results, also results similar produced estimated error variance \\(\\widehat{\\sigma}^2\\) model, reported (except highest-degree polynomials, CV results clearly suggest -fitting).","code":"for (p in 1:10){   assign(paste0(\"m.\", p),          lm(mpg ~ poly(horsepower, p), data=Auto)) } objects(pattern=\"m\\\\.[0-9]\") #>  [1] \"m.1\"  \"m.10\" \"m.2\"  \"m.3\"  \"m.4\"  \"m.5\"  \"m.6\"  \"m.7\"  \"m.8\"  \"m.9\" summary(m.2) # for example, the quadratic fit #>  #> Call: #> lm(formula = mpg ~ poly(horsepower, p), data = Auto) #>  #> Residuals: #>     Min      1Q  Median      3Q     Max  #> -14.714  -2.594  -0.086   2.287  15.896  #>  #> Coefficients: #>                      Estimate Std. Error t value Pr(>|t|)     #> (Intercept)            23.446      0.221   106.1   <2e-16 *** #> poly(horsepower, p)1 -120.138      4.374   -27.5   <2e-16 *** #> poly(horsepower, p)2   44.090      4.374    10.1   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 4.37 on 389 degrees of freedom #> Multiple R-squared:  0.688,  Adjusted R-squared:  0.686  #> F-statistic:  428 on 2 and 389 DF,  p-value: <2e-16 # 10-fold CV cv.auto.10 <- cv(models(m.1, m.2, m.3, m.4, m.5,                      m.6, m.7, m.8, m.9, m.10),               data=Auto, seed=2120) cv.auto.10[1:2] # for the linear and quadratic models #>  #> Model model.1: #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 24.246 #> bias-adjusted cross-validation criterion = 24.23 #> full-sample criterion = 23.944  #>  #> Model model.2: #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 19.346 #> bias-adjusted cross-validation criterion = 19.327 #> full-sample criterion = 18.985  # LOO CV cv.auto.loo <- cv(models(m.1, m.2, m.3, m.4, m.5,                         m.6, m.7, m.8, m.9, m.10),                  data=Auto, k=\"loo\") cv.auto.loo[1:2] # linear and quadratic models #>  #> Model model.1: #> n-Fold Cross Validation #> method: hatvalues #> cross-validation criterion = 24.232 #> Model model.2: #> n-Fold Cross Validation #> method: hatvalues #> cross-validation criterion = 19.248 cv.mse.10 <- sapply(cv.auto.10, function(x) x[[\"adj CV crit\"]]) cv.mse.loo <- sapply(cv.auto.loo, function(x) x[[\"CV crit\"]]) plot(c(1, 10), range(cv.mse.10, cv.mse.loo), type=\"n\",      xlab=\"Degree of polynomial, p\",      ylab=\"Cross-Validated MSE\") lines(1:10, cv.mse.10, lwd=2, lty=1, col=2, pch=16, type=\"b\") lines(1:10, cv.mse.loo, lwd=2, lty=2, col=3, pch=17, type=\"b\") legend(\"topright\", inset=0.02,        legend=c(\"10-Fold CV\", \"LOO CV\"),        lwd=2, lty=2:1, col=3:2, pch=17:16) plot(cv.auto.10, main=\"Polynomial Regressions, 10-Fold CV\",      axis.args=list(labels=1:10), xlab=\"Degree of Polynomial, p\") plot(cv.auto.loo, main=\"Polynomial Regressions, LOO CV\",      axis.args=list(labels=1:10), xlab=\"Degree of Polynomial, p\")"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"logistic-regression-for-the-mroz-data","dir":"Articles","previous_headings":"Examples","what":"Logistic regression for the Mroz data","title":"Cross-validation of regression models","text":"Mroz data set carData package (associated Fox & Weisberg, 2019) used several authors illustrate binary logistic regression; see, particular Fox & Weisberg (2019). data originally drawn U.S. Panel Study Income Dynamics pertain married women. cases data set: response variable logistic regression lfp, labor-force participation, factor coded \"yes\" \"\". remaining variables predictors: k5, number children 5 years old younger woman’s household; k618, number children 6 18 years old; age, years; wc, wife’s college attendance, \"yes\" \"\"; hc, husband’s college attendance; lwg, woman’s log wage rate employed, imputed wage rate, (variable Fox & Weisberg, 2019 show problematically defined); inc, family income, $1000s, exclusive wife’s income. use glm() function fit binary logistic regression Mroz data: addition usually summary output GLM, show result applying BayesRule() function cv package predictions derived fitted model. Bayes rule, predicts “success” binary regression model fitted probability success [.e., \\(\\phi = \\Pr(y = 1)\\)] \\(\\widehat{\\phi} \\ge .5\\) “failure” \\(\\widehat{\\phi} \\lt .5\\).4 first argument BayesRule() binary {0, 1} response, second argument predicted probability success. BayesRule() returns proportion predictions error, appropriate “cost” function. example, fitted logistic regression incorrectly predicts 31% responses; expect estimate optimistic given model used “predict” data fit. \"glm\" method cv() largely similar \"lm\" method, although default algorithm, selected explicitly method=\"exact\", refits model fold removed (thus equivalent method=\"naive\" \"lm\" models). generalized linear models, method=\"Woodbury\" (LOO CV) method=\"hatvalues\" provide approximate results (see last section vignette details): ensure two methods use 10 folds, specify seed R’s random-number generator explicitly; , common experience, \"exact\" \"Woodbury\" algorithms produce nearly identical results. CV estimates prediction error slightly higher estimate based cases. results applying LOO CV Mroz model, using exact approximate methods: number decimal digits shown, three methods produce identical results example. linear models, report timings various cv() methods computation LOO CV well cv.glm() function boot package (, recall, refits model case removed, thus comparable cv() method=\"exact\"): substantial time penalty associated exact computations.","code":"data(\"Mroz\", package=\"carData\") head(Mroz, 3) #>   lfp k5 k618 age wc hc    lwg   inc #> 1 yes  1    0  32 no no 1.2102 10.91 #> 2 yes  0    2  30 no no 0.3285 19.50 #> 3 yes  1    3  35 no no 1.5141 12.04 tail(Mroz, 3) #>     lfp k5 k618 age wc hc     lwg    inc #> 751  no  0    0  43 no no 0.88814  9.952 #> 752  no  0    0  60 no no 1.22497 24.984 #> 753  no  0    3  39 no no 0.85321 28.363 m.mroz <- glm(lfp ~ ., data=Mroz, family=binomial) summary(m.mroz) #>  #> Call: #> glm(formula = lfp ~ ., family = binomial, data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  3.18214    0.64438    4.94  7.9e-07 *** #> k5          -1.46291    0.19700   -7.43  1.1e-13 *** #> k618        -0.06457    0.06800   -0.95  0.34234     #> age         -0.06287    0.01278   -4.92  8.7e-07 *** #> wcyes        0.80727    0.22998    3.51  0.00045 *** #> hcyes        0.11173    0.20604    0.54  0.58762     #> lwg          0.60469    0.15082    4.01  6.1e-05 *** #> inc         -0.03445    0.00821   -4.20  2.7e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  905.27  on 745  degrees of freedom #> AIC: 921.3 #>  #> Number of Fisher Scoring iterations: 4  BayesRule(ifelse(Mroz$lfp == \"yes\", 1, 0),            fitted(m.mroz, type=\"response\")) #> [1] 0.30677 cv(m.mroz, criterion=BayesRule, seed=248) #> R RNG seed set to 248 #> 10-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.32404 #> bias-adjusted cross-validation criterion = 0.31952 #> full-sample criterion = 0.30677  cv(m.mroz, criterion=BayesRule, seed=248, method=\"Woodbury\") #> R RNG seed set to 248 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32404 #> bias-adjusted cross-validation criterion = 0.31926 #> full-sample criterion = 0.30677 cv(m.mroz, k=\"loo\", criterion=BayesRule) #> n-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.32005 #> bias-adjusted cross-validation criterion = 0.3183 #> full-sample criterion = 0.30677  cv(m.mroz, k=\"loo\", criterion=BayesRule, method=\"Woodbury\") #> n-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32005 #> bias-adjusted cross-validation criterion = 0.3183 #> full-sample criterion = 0.30677  cv(m.mroz, k=\"loo\", criterion=BayesRule, method=\"hatvalues\") #> n-Fold Cross Validation #> method: hatvalues #> criterion: BayesRule #> cross-validation criterion = 0.32005 microbenchmark::microbenchmark(   hatvalues=cv(m.mroz, k=\"loo\", criterion=BayesRule, method=\"hatvalues\"),   Woodbury=cv(m.mroz, k=\"loo\", criterion=BayesRule, method=\"Woodbury\"),   exact=cv(m.mroz, k=\"loo\", criterion=BayesRule),   cv.glm=boot::cv.glm(Mroz, m.mroz,                cost=BayesRule),   times=10) #> Unit: milliseconds #>       expr       min        lq      mean    median        uq       max neval #>  hatvalues    3.0094    3.0876    3.1808    3.1096    3.1961    3.7724    10 #>   Woodbury   85.5894   86.3915   88.5377   88.2420   90.4306   91.5510    10 #>      exact 3841.6784 3911.1349 3963.0170 3971.3942 4032.2979 4093.1560    10 #>     cv.glm 4655.3505 4689.3870 4755.8227 4767.8682 4788.2193 4869.5470    10"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"cross-validating-mixed-effects-models","dir":"Articles","previous_headings":"Examples","what":"Cross-validating mixed-effects models","title":"Cross-validation of regression models","text":"fundamental analogy cross-validation collection new data. , predicting response fold model fit data folds like using model fit data predict response new cases values predictors new cases. explained, application idea independently sampled cases straightforward—simply partition data random folds equal size leave fold turn, , case LOO CV, simply omit case turn. contrast, mixed-effects models fit dependent data, cases clustered, hierarchical data, clusters comprise higher-level units (e.g., students clustered schools), longitudinal data, clusters individuals cases repeated observations individuals time.5 can think two approaches applying cross-validation clustered data: Treat CV analogous predicting response one cases newly observed cluster. instance, folds comprise one whole clusters; refit model cases clusters current fold removed; predict response cases clusters current fold. predictions based fixed effects random effects omitted clusters presumably unknown, data cases newly observed clusters. Treat CV analogous predicting response newly observed case existing cluster. instance, folds comprise one individual cases, predictions can use fixed random effects.","code":""},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"example-the-high-school-and-beyond-data","dir":"Articles","previous_headings":"Examples","what":"Example: The High-School and Beyond data","title":"Cross-validation of regression models","text":"Following use Raudenbush & Bryk (2002), data 1982 High School Beyond (HSB) survey become staple literature mixed-effects models. HSB data used Fox & Weisberg (2019, sec. 7.2.2) illustrate application linear mixed models hierarchical data, ’ll closely follow example . HSB data included MathAchieve MathAchSchool data sets nlme package (Pinheiro & Bates, 2000). MathAchieve includes individual-level data 7185 students 160 high schools, MathAchSchool includes school-level data: first students school number 1224 last school 9586. ’ll use School, SES (students’ socioeconomic status), MathAch (score standardized math-achievement test) variables MathAchieve data set, Sector (\"Catholic\" \"Public\") MathAchSchool data set. data-management required fitting mixed-effects model HSB data, use dplyr package (Wickham, François, Henry, Müller, & Vaughan, 2023): process, created two new school-level variables: meanses, average SES students school; cses, school-average SES centered mean. details, see Fox & Weisberg (2019, sec. 7.2.2). Still following Fox Weisberg, proceed use lmer() function lme4 package (Bates, Mächler, Bolker, & Walker, 2015) fit mixed model math achievement HSB data: can cross-validate cluster (.e., school) level, case (.e., student) level, cluster-level CV, clusterVariables argument tells cv() clusters defined. one clustering variable, say classes within schools, provided character vector variable names: clusterVariables = c(\"school\", \"class\"). cluster-level CV, default k = \"loo\", , leave one cluster time; instead specify k = 10 folds clusters, fold therefore comprising \\(160/10 = 16\\) schools. clusterVariables argument omitted, case-level CV employed, k = 10 folds default, \\(7185/10 \\approx 719\\) students. Notice one 10 models refit fold removed failed converge. Convergence problems common mixed-effects modeling. apparent issue estimated variance component close equal 0, boundary parameter space. shouldn’t disqualify fitted model kind prediction required cross-validation. also cv() method linear mixed models fit lme() function nlme package, arguments cv() case model fit lmer() glmer(). illustrate mixed model fit HSB data: used random-number generator seeds previous example cross-validating model fit lmer(), folds employed cases.6 estimated covariance components fixed effects summary output differ slightly lmer() lme() solutions, although functions seek maximize REML criterion. , course, expected different algorithms used numerical optimization. precision reported, cluster-level CV results lmer() lme() models identical, case-level CV results similar identical.","code":"data(\"MathAchieve\", package=\"nlme\") dim(MathAchieve) #> [1] 7185    6 head(MathAchieve, 3) #> Grouped Data: MathAch ~ SES | School #>   School Minority    Sex    SES MathAch MEANSES #> 1   1224       No Female -1.528   5.876  -0.428 #> 2   1224       No Female -0.588  19.708  -0.428 #> 3   1224       No   Male -0.528  20.349  -0.428 tail(MathAchieve, 3) #> Grouped Data: MathAch ~ SES | School #>      School Minority    Sex    SES MathAch MEANSES #> 7183   9586       No Female  1.332  19.641   0.627 #> 7184   9586       No Female -0.008  16.241   0.627 #> 7185   9586       No Female  0.792  22.733   0.627  data(\"MathAchSchool\", package=\"nlme\") dim(MathAchSchool) #> [1] 160   7 head(MathAchSchool, 2) #>      School Size Sector PRACAD DISCLIM HIMINTY MEANSES #> 1224   1224  842 Public   0.35   1.597       0  -0.428 #> 1288   1288 1855 Public   0.27   0.174       0   0.128 tail(MathAchSchool, 2) #>      School Size   Sector PRACAD DISCLIM HIMINTY MEANSES #> 9550   9550 1532   Public   0.45   0.791       0   0.059 #> 9586   9586  262 Catholic   1.00  -2.416       0   0.627 library(\"dplyr\") #>  #> Attaching package: 'dplyr' #> The following objects are masked from 'package:stats': #>  #>     filter, lag #> The following objects are masked from 'package:base': #>  #>     intersect, setdiff, setequal, union MathAchieve %>% group_by(School) %>%   summarize(mean.ses = mean(SES)) -> Temp Temp <- merge(MathAchSchool, Temp, by=\"School\") HSB <- merge(Temp[, c(\"School\", \"Sector\", \"mean.ses\")],              MathAchieve[, c(\"School\", \"SES\", \"MathAch\")], by=\"School\") names(HSB) <- tolower(names(HSB))  HSB$cses <- with(HSB, ses - mean.ses) library(\"lme4\") #> Loading required package: Matrix hsb.lmer <- lmer(mathach ~ mean.ses*cses + sector*cses                    + (cses | school), data=HSB) summary(hsb.lmer, correlation=FALSE) #> Linear mixed model fit by REML ['lmerMod'] #> Formula: mathach ~ mean.ses * cses + sector * cses + (cses | school) #>    Data: HSB #>  #> REML criterion at convergence: 46504 #>  #> Scaled residuals:  #>    Min     1Q Median     3Q    Max  #> -3.159 -0.723  0.017  0.754  2.958  #>  #> Random effects: #>  Groups   Name        Variance Std.Dev. Corr #>  school   (Intercept)  2.380   1.543         #>           cses         0.101   0.318    0.39 #>  Residual             36.721   6.060         #> Number of obs: 7185, groups:  school, 160 #>  #> Fixed effects: #>                     Estimate Std. Error t value #> (Intercept)           12.128      0.199   60.86 #> mean.ses               5.333      0.369   14.45 #> cses                   2.945      0.156   18.93 #> sectorCatholic         1.227      0.306    4.00 #> mean.ses:cses          1.039      0.299    3.48 #> cses:sectorCatholic   -1.643      0.240   -6.85 cv(hsb.lmer, k=10, clusterVariables=\"school\", seed=5240) #> R RNG seed set to 5240 #> 10-Fold Cross Validation based on 160 {school} clusters #> cross-validation criterion = 39.133 #> bias-adjusted cross-validation criterion = 39.125 #> full-sample criterion = 39.006 cv(hsb.lmer, seed=1575) #> R RNG seed set to 1575 #> Warning in checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv, : #> Model failed to converge with max|grad| = 0.00587141 (tol = 0.002, component 1) #> boundary (singular) fit: see help('isSingular') #> 10-Fold Cross Validation #> cross-validation criterion = 37.445 #> bias-adjusted cross-validation criterion = 37.338 #> full-sample criterion = 36.068 library(\"nlme\") #>  #> Attaching package: 'nlme' #> The following object is masked from 'package:lme4': #>  #>     lmList #> The following object is masked from 'package:dplyr': #>  #>     collapse #> The following object is masked from 'package:cv': #>  #>     getResponse hsb.lme <- lme(mathach ~ mean.ses*cses + sector*cses,                  random = ~ cses | school, data=HSB,                control=list(opt=\"optim\")) summary(hsb.lme) #> Linear mixed-effects model fit by REML #>   Data: HSB  #>     AIC   BIC logLik #>   46525 46594 -23252 #>  #> Random effects: #>  Formula: ~cses | school #>  Structure: General positive-definite, Log-Cholesky parametrization #>             StdDev   Corr   #> (Intercept) 1.541177 (Intr) #> cses        0.018174 0.006  #> Residual    6.063492        #>  #> Fixed effects:  mathach ~ mean.ses * cses + sector * cses  #>                       Value Std.Error   DF t-value p-value #> (Intercept)         12.1282   0.19920 7022  60.886   0e+00 #> mean.ses             5.3367   0.36898  157  14.463   0e+00 #> cses                 2.9421   0.15122 7022  19.456   0e+00 #> sectorCatholic       1.2245   0.30611  157   4.000   1e-04 #> mean.ses:cses        1.0444   0.29107 7022   3.588   3e-04 #> cses:sectorCatholic -1.6421   0.23312 7022  -7.044   0e+00 #>  Correlation:  #>                     (Intr) men.ss cses   sctrCt mn.ss: #> mean.ses             0.256                             #> cses                 0.000  0.000                      #> sectorCatholic      -0.699 -0.356  0.000               #> mean.ses:cses        0.000  0.000  0.295  0.000        #> cses:sectorCatholic  0.000  0.000 -0.696  0.000 -0.351 #>  #> Standardized Within-Group Residuals: #>       Min        Q1       Med        Q3       Max  #> -3.170106 -0.724877  0.014892  0.754263  2.965498  #>  #> Number of Observations: 7185 #> Number of Groups: 160  cv(hsb.lme, k=10, clusterVariables=\"school\", seed=5240) #> R RNG seed set to 5240 #> 10-Fold Cross Validation based on 160 {school} clusters #> cross-validation criterion = 39.133 #> bias-adjusted cross-validation criterion = 39.125 #> full-sample criterion = 39.006  cv(hsb.lme, seed=1575) #> R RNG seed set to 1575 #> 10-Fold Cross Validation #> cross-validation criterion = 37.442 #> bias-adjusted cross-validation criterion = 37.402 #> full-sample criterion = 36.147"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"replicating-cross-validation","dir":"Articles","previous_headings":"","what":"Replicating cross-validation","title":"Cross-validation of regression models","text":"Assuming number cases \\(n\\) multiple number folds \\(k\\)—slightly simplifying assumption—number possible partitions cases folds \\(\\frac{n!}{[(n/k)!]^k}\\), number grows large quickly. example, \\(n = 10\\) \\(k = 5\\), folds size \\(n/k = 2\\), \\(113,400\\) possible partitions; \\(n=100\\) \\(k=5\\), \\(n/k = 20\\), still small problem, number possible partitions truly astronomical, \\(1.09\\times 10^{66}\\). partition folds ’s employed selected randomly, resulting CV criterion estimates subject sampling error. (exception LOO cross-validation, random.) get sense magnitude sampling error, can repeat CV procedure different randomly selected partitions folds. CV functions cv package capable repeated cross-validation, number repetitions controlled reps argument, defaults 1. , example, 10-fold CV Mroz logistic regression, repeated 5 times: reps > 1, result returned cv() object class \"cvList\"—literally list \"cv\" objects. results reported repetition averaged across repetitions, standard deviations CV criterion biased-adjusted CV criterion given parentheses. example, therefore little variation across repetitions, increasing confidence reliability results. Notice seed ’s set cv() command pertains first repetition seeds remaining repetitions selected pseudo-randomly.7 Setting first seed, however, makes entire process easily replicable, seed repetition stored corresponding element \"cvList\" object (isn’t, however, saved example). ’s also possible replicate CV comparing competing models via cv() method \"modList\" objects. Recall comparison polynomial regressions varying degree fit Auto data; performed 10-fold CV 10 models. , replicate process 5 times model graph results: Replicated cross-validated 10-fold CV function polynomial degree, \\(p\\) graph shows average CV criterion range competing models.","code":"cv(m.mroz, criterion=BayesRule, seed=248, reps=5,     method=\"Woodbury\") #> R RNG seed set to 248 #> R RNG seed set to 68134 #> R RNG seed set to 767359 #> R RNG seed set to 556270 #> R RNG seed set to 882966 #>  #> Replicate 1: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32005 #> bias-adjusted cross-validation criterion = 0.31301 #> full-sample criterion = 0.30677  #>  #> Replicate 2: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.31607 #> bias-adjusted cross-validation criterion = 0.3117 #> full-sample criterion = 0.30677  #>  #> Replicate 3: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.31474 #> bias-adjusted cross-validation criterion = 0.30862 #> full-sample criterion = 0.30677  #>  #> Replicate 4: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32404 #> bias-adjusted cross-validation criterion = 0.31807 #> full-sample criterion = 0.30677  #>  #> Replicate 5: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.32404 #> bias-adjusted cross-validation criterion = 0.31926 #> full-sample criterion = 0.30677  #>  #> Average: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: BayesRule #> cross-validation criterion = 0.31983 (0.003887) #> bias-adjusted cross-validation criterion = 0.31394 (0.0040093) #> full-sample criterion = 0.30677 cv.auto.reps <- cv(models(m.1, m.2, m.3, m.4, m.5,                         m.6, m.7, m.8, m.9, m.10),                  data=Auto, seed=8004, reps=5) plot(cv.auto.reps)"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"a-preliminary-example","dir":"Articles","previous_headings":"Cross-validating model selection","what":"A preliminary example","title":"Cross-validation of regression models","text":"Hastie, Tibshirani, & Friedman (2009, sec. 7.10.2: “Wrong Right Way Cross-validation”) explain, whole data used select fine-tune statistical model, subsequent cross-validation model intrinsically misleading, model selected fit whole data, including part data remains fold removed. following example similar spirit one employed Hastie et al. (2009). Suppose randomly generate \\(n = 1000\\) independent observations response variable variable \\(y \\sim N(\\mu = 10, \\sigma^2 = 0)\\), independently sample \\(1000\\) observations \\(p = 100\\) “predictors,” \\(x_1, \\ldots, x_{100}\\), \\(x_j \\sim N(0, 1)\\). response nothing predictors population linear-regression model \\(y_i = \\alpha + \\beta_1 x_{i1} + \\cdots + \\beta_{100} x_{,100} + \\varepsilon_i\\) \\(\\alpha = 10\\) \\(\\beta_j = 0\\). Least-squares provides accurate estimates regression constant \\(\\alpha = 10\\) error variance \\(\\sigma^2 = 1\\) “null model” including regression constant; moreover, omnibus \\(F\\)-test correct null hypothesis \\(\\beta\\)s 0 “full model” 100 \\(x\\)s associated large \\(p\\)-value: Next, using stepAIC() function MASS package (Venables & Ripley, 2002), let us perform forward stepwise regression select “best” model, starting null model, using AIC model-selection criterion (see help page stepAIC() details):8 resulting model 15 predictors, modest \\(R^2 = .044\\), small \\(p\\)-value omnibus \\(F\\)-test (, course, entirely spurious data used select test model). MSE selected model smaller true error variance \\(\\sigma^2 = 1\\), estimated error variance selected model, \\(\\widehat{\\sigma}^2 = 0.973^2 = 0.947\\). cross-validate selected model, also obtain optimistic estimate predictive power: cvSelect() function cv package allows us cross-validate whole model-selection procedure. first argument cvSelect() model-selection function capable refitting model fold omitted returning CV criterion. selectStepAIC() function, also cv based stepAIC(), suitable use cvSelect(): arguments cvSelect() : data, data set model fit; seed, optional seed R’s pseudo-random-number generator; cv(), seed isn’t supplied user, seed randomly selected saved; additional arguments required model-selection function, starting model argument, direction model selection, scope models considered (model regression constant model 100 predictors). default, cvSelect() performs 10-fold CV, produces estimate MSE model-selection procedure even larger true error variance, \\(\\sigma^2 = 1\\). Also default, number folds 10 fewer, cvSelect() saves coefficients selected models. example, compareFolds() function reveals variables retained model-selection process several folds quite different:","code":"set.seed(24361) # for reproducibility D <- data.frame(   y = rnorm(1000, mean=10),   X = matrix(rnorm(1000*100), 1000, 100) ) head(D[, 1:6]) #>         y      X.1      X.2      X.3       X.4       X.5 #> 1 10.0316 -1.23886 -0.26487 -0.03539 -2.576973  0.811048 #> 2  9.6650  0.12287 -0.17744  0.37290 -0.935138  0.628673 #> 3 10.0232 -0.95052 -0.73487 -1.05978  0.882944  0.023918 #> 4  8.9910  1.13571  0.32411  0.11037  1.376303 -0.422114 #> 5  9.0712  1.49474  1.87538  0.10575  0.292140 -0.184568 #> 6 11.3493 -0.18453 -0.78037 -1.23804 -0.010949  0.691034 m.full <- lm(y ~ ., data=D) m.null <- lm(y ~ 1, data=D) anova(m.null, m.full) #> Analysis of Variance Table #>  #> Model 1: y ~ 1 #> Model 2: y ~ X.1 + X.2 + X.3 + X.4 + X.5 + X.6 + X.7 + X.8 + X.9 + X.10 +  #>     X.11 + X.12 + X.13 + X.14 + X.15 + X.16 + X.17 + X.18 + X.19 +  #>     X.20 + X.21 + X.22 + X.23 + X.24 + X.25 + X.26 + X.27 + X.28 +  #>     X.29 + X.30 + X.31 + X.32 + X.33 + X.34 + X.35 + X.36 + X.37 +  #>     X.38 + X.39 + X.40 + X.41 + X.42 + X.43 + X.44 + X.45 + X.46 +  #>     X.47 + X.48 + X.49 + X.50 + X.51 + X.52 + X.53 + X.54 + X.55 +  #>     X.56 + X.57 + X.58 + X.59 + X.60 + X.61 + X.62 + X.63 + X.64 +  #>     X.65 + X.66 + X.67 + X.68 + X.69 + X.70 + X.71 + X.72 + X.73 +  #>     X.74 + X.75 + X.76 + X.77 + X.78 + X.79 + X.80 + X.81 + X.82 +  #>     X.83 + X.84 + X.85 + X.86 + X.87 + X.88 + X.89 + X.90 + X.91 +  #>     X.92 + X.93 + X.94 + X.95 + X.96 + X.97 + X.98 + X.99 + X.100 #>   Res.Df RSS  Df Sum of Sq    F Pr(>F) #> 1    999 974                           #> 2    899 888 100      85.2 0.86   0.82  summary(m.null) #>  #> Call: #> lm(formula = y ~ 1, data = D) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -3.458 -0.681  0.019  0.636  2.935  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   9.9370     0.0312     318   <2e-16 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 0.987 on 999 degrees of freedom m.select <- MASS::stepAIC(m.null,                           direction=\"forward\", trace=FALSE,                           scope=list(lower=~1, upper=formula(m.full))) summary(m.select) #>  #> Call: #> lm(formula = y ~ X.99 + X.90 + X.87 + X.40 + X.65 + X.91 + X.53 +  #>     X.45 + X.31 + X.56 + X.61 + X.60 + X.46 + X.35 + X.92, data = D) #>  #> Residuals: #>    Min     1Q Median     3Q    Max  #> -3.262 -0.645  0.024  0.641  3.118  #>  #> Coefficients: #>             Estimate Std. Error t value Pr(>|t|)     #> (Intercept)   9.9372     0.0310  320.80   <2e-16 *** #> X.99         -0.0910     0.0308   -2.95   0.0032 **  #> X.90         -0.0820     0.0314   -2.62   0.0090 **  #> X.87         -0.0694     0.0311   -2.24   0.0256 *   #> X.40         -0.0476     0.0308   -1.55   0.1221     #> X.65         -0.0552     0.0315   -1.76   0.0795 .   #> X.91          0.0524     0.0308    1.70   0.0894 .   #> X.53         -0.0492     0.0305   -1.61   0.1067     #> X.45          0.0554     0.0318    1.74   0.0818 .   #> X.31          0.0452     0.0311    1.46   0.1457     #> X.56          0.0543     0.0327    1.66   0.0972 .   #> X.61         -0.0508     0.0317   -1.60   0.1091     #> X.60         -0.0513     0.0319   -1.61   0.1083     #> X.46          0.0516     0.0327    1.58   0.1153     #> X.35          0.0470     0.0315    1.49   0.1358     #> X.92          0.0443     0.0310    1.43   0.1533     #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> Residual standard error: 0.973 on 984 degrees of freedom #> Multiple R-squared:  0.0442, Adjusted R-squared:  0.0296  #> F-statistic: 3.03 on 15 and 984 DF,  p-value: 8.34e-05 mse(D$y, fitted(m.select)) #> [1] 0.93063 cv(m.select, seed=2529) #> R RNG seed set to 2529 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 0.95937 #> bias-adjusted cross-validation criterion = 0.95785 #> full-sample criterion = 0.93063 cv.select <- cvSelect(selectStepAIC, data=D, seed=3791,                       model=m.null, direction=\"forward\",                       scope=list(lower=~1,                                   upper=formula(m.full))) #> R RNG seed set to 3791 cv.select #> 10-Fold Cross Validation #> cross-validation criterion = 1.0687 #> bias-adjusted cross-validation criterion = 1.0612 #> full-sample criterion = 0.93063 compareFolds(cv.select) #>         (Intercept)    X.87    X.90    X.99    X.91    X.54    X.53    X.56 #> Fold 1       9.9187 -0.0615 -0.0994 -0.0942  0.0512  0.0516                 #> Fold 2       9.9451 -0.0745 -0.0899 -0.0614          0.0587          0.0673 #> Fold 3       9.9423 -0.0783 -0.0718 -0.0987  0.0601                  0.0512 #> Fold 4       9.9410 -0.0860 -0.0831 -0.0867  0.0570         -0.0508         #> Fold 5       9.9421 -0.0659 -0.0849 -0.1004  0.0701  0.0511 -0.0487  0.0537 #> Fold 6       9.9633 -0.0733 -0.0874 -0.0960  0.0555  0.0629 -0.0478         #> Fold 7       9.9279 -0.0618 -0.0960 -0.0838  0.0533         -0.0464         #> Fold 8       9.9453 -0.0610 -0.0811 -0.0818          0.0497 -0.0612  0.0560 #> Fold 9       9.9173 -0.0663 -0.0894 -0.1100  0.0504  0.0524          0.0747 #> Fold 10      9.9449 -0.0745 -0.0906 -0.0891  0.0535  0.0482 -0.0583  0.0642 #>            X.40    X.45    X.65    X.68    X.92    X.15    X.26    X.46    X.60 #> Fold 1                  -0.0590                 -0.0456  0.0658  0.0608         #> Fold 2                                   0.0607          0.0487                 #> Fold 3  -0.0496         -0.0664          0.0494                                 #> Fold 4  -0.0597  0.0579 -0.0531          0.0519 -0.0566                 -0.0519 #> Fold 5                           0.0587                          0.0527 -0.0603 #> Fold 6  -0.0596  0.0552          0.0474                                         #> Fold 7           0.0572          0.0595                                         #> Fold 8           0.0547 -0.0617  0.0453  0.0493 -0.0613  0.0591  0.0703 -0.0588 #> Fold 9  -0.0552  0.0573 -0.0635  0.0492         -0.0513  0.0484         -0.0507 #> Fold 10 -0.0558                          0.0529                  0.0710         #>            X.61     X.8    X.28    X.29    X.31    X.35    X.70    X.89    X.17 #> Fold 1  -0.0490          0.0616 -0.0537                  0.0638                 #> Fold 2           0.0671                  0.0568                  0.0523         #> Fold 3  -0.0631          0.0616                                                 #> Fold 4           0.0659         -0.0549          0.0527                  0.0527 #> Fold 5           0.0425                  0.0672  0.0613          0.0493         #> Fold 6           0.0559         -0.0629  0.0498          0.0487                 #> Fold 7                                                           0.0611  0.0472 #> Fold 8  -0.0719                                          0.0586                 #> Fold 9                   0.0525                                                 #> Fold 10 -0.0580                                  0.0603                         #>            X.25     X.4    X.64    X.81    X.97    X.11     X.2    X.33    X.47 #> Fold 1                                   0.0604          0.0575                 #> Fold 2   0.0478          0.0532  0.0518                                         #> Fold 3                           0.0574                          0.0473         #> Fold 4                   0.0628                                                 #> Fold 5   0.0518                                                                 #> Fold 6                                           0.0521                         #> Fold 7           0.0550                                                         #> Fold 8                                                                          #> Fold 9                                   0.0556                          0.0447 #> Fold 10          0.0516                                                         #>             X.6    X.72    X.73    X.77    X.79 X.88 #> Fold 1   0.0476                                      #> Fold 2                   0.0514                      #> Fold 3                                               #> Fold 4                                  -0.0473      #> Fold 5           0.0586                         0.07 #> Fold 6                          -0.0489              #> Fold 7                                               #> Fold 8                                               #> Fold 9                                               #> Fold 10"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"mrozs-logistic-regression-revisited","dir":"Articles","previous_headings":"Cross-validating model selection","what":"Mroz’s logistic regression revisited","title":"Cross-validation of regression models","text":"contrasting example apply model selection Mroz’s logistic regression married women’s labor-force participation. First, recall logistic regression model fit Mroz data: Applying stepwise model selection Mroz’s logistic regression, using BIC model-selection criterion (via argument k=log(nrow(Mroz)) stepAIC()) selects 5 7 original predictors: Bayes rule applied selected model misclassifies 32% cases Mroz data. Cross-validating selected model produces similar, slightly larger, estimate misclassification, 33%: estimate predictive performance optimistic? proceed apply model-selection procedure cross-validation, producing less result: Setting AIC=FALSE call cvSelect() uses BIC rather AIC model-selection criterion. turns , exactly predictors selected 10 folds omitted, several coefficient estimates similar, show using compareFolds(): example, therefore, appear obtain realistic estimate model performance directly selected model, little added uncertainty induced model selection.","code":"summary(m.mroz) #>  #> Call: #> glm(formula = lfp ~ ., family = binomial, data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)  3.18214    0.64438    4.94  7.9e-07 *** #> k5          -1.46291    0.19700   -7.43  1.1e-13 *** #> k618        -0.06457    0.06800   -0.95  0.34234     #> age         -0.06287    0.01278   -4.92  8.7e-07 *** #> wcyes        0.80727    0.22998    3.51  0.00045 *** #> hcyes        0.11173    0.20604    0.54  0.58762     #> lwg          0.60469    0.15082    4.01  6.1e-05 *** #> inc         -0.03445    0.00821   -4.20  2.7e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  905.27  on 745  degrees of freedom #> AIC: 921.3 #>  #> Number of Fisher Scoring iterations: 4 m.mroz.sel <- MASS::stepAIC(m.mroz, k=log(nrow(Mroz)),                             trace=FALSE) summary(m.mroz.sel) #>  #> Call: #> glm(formula = lfp ~ k5 + age + wc + lwg + inc, family = binomial,  #>     data = Mroz) #>  #> Coefficients: #>             Estimate Std. Error z value Pr(>|z|)     #> (Intercept)   2.9019     0.5429    5.35  9.0e-08 *** #> k5           -1.4318     0.1932   -7.41  1.3e-13 *** #> age          -0.0585     0.0114   -5.13  2.9e-07 *** #> wcyes         0.8724     0.2064    4.23  2.4e-05 *** #> lwg           0.6157     0.1501    4.10  4.1e-05 *** #> inc          -0.0337     0.0078   -4.32  1.6e-05 *** #> --- #> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 #>  #> (Dispersion parameter for binomial family taken to be 1) #>  #>     Null deviance: 1029.75  on 752  degrees of freedom #> Residual deviance:  906.46  on 747  degrees of freedom #> AIC: 918.5 #>  #> Number of Fisher Scoring iterations: 3 BayesRule(Mroz$lfp == \"yes\",           predict(m.mroz.sel, type=\"response\")) #> [1] 0.31873 cv(m.mroz.sel, criterion=BayesRule, seed=345266) #> R RNG seed set to 345266 #> 10-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.33068 #> bias-adjusted cross-validation criterion = 0.33332 #> full-sample criterion = 0.31873 m.mroz.sel.cv <- cvSelect(selectStepAIC, Mroz,                            seed=6681,                           criterion=BayesRule,                           model=m.mroz,                           AIC=FALSE) #> R RNG seed set to 6681 m.mroz.sel.cv #> 10-Fold Cross Validation #> cross-validation criterion = 0.33068 #> bias-adjusted cross-validation criterion = 0.33452 #> full-sample criterion = 0.31873 compareFolds(m.mroz.sel.cv) #>         (Intercept)     age     inc      k5     lwg wcyes #> Fold 1       2.5014 -0.0454 -0.0388 -1.3613  0.5653  0.85 #> Fold 2       3.0789 -0.0659 -0.0306 -1.5335  0.6923  0.79 #> Fold 3       3.0141 -0.0595 -0.0305 -1.3994  0.5428  0.86 #> Fold 4       2.7251 -0.0543 -0.0354 -1.4474  0.6298  1.09 #> Fold 5       2.7617 -0.0566 -0.0320 -1.4752  0.6324  0.74 #> Fold 6       3.0234 -0.0621 -0.0348 -1.4537  0.6618  0.94 #> Fold 7       2.9615 -0.0600 -0.0351 -1.4127  0.5835  0.97 #> Fold 8       2.9598 -0.0603 -0.0329 -1.3865  0.6210  0.69 #> Fold 9       3.2481 -0.0650 -0.0381 -1.4138  0.6093  0.94 #> Fold 10      2.7724 -0.0569 -0.0295 -1.4503  0.6347  0.85"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"cross-validating-choice-of-transformations-in-regression","dir":"Articles","previous_headings":"Cross-validating model selection","what":"Cross-validating choice of transformations in regression","title":"Cross-validation of regression models","text":"cv package also provides cvSelect() procedure, selectTrans(), choosing transformations predictors response regression. background: Weisberg (2014, sec. 8.2) explains, technical advantages (numeric) predictors linear regression analysis linearly related. predictors aren’t linearly related, relationships can often straightened power transformations. Transformations can selected graphical examination data, analytic methods. relationships predictors linearized, can advantageous similarly transform response variable towards normality. Selecting transformations analytically raises possibility automating process, required cross-validation. One , principle, apply graphical methods select transformations fold, data analyst couldn’t forget choices made previous folds, process wouldn’t really applied independently folds. illustrate, adapt example appearing several places Fox & Weisberg (2019) (example Chapter 3 transforming data), using data prestige characteristics 102 Canadian occupations circa 1970. data Prestige data frame carData package: variables Prestige data set : education: average years education incumbents occupation, 1971 Canadian Census. income: average dollars annual income occupation, Census. women: percentage occupational incumbents women, also Census. prestige: average prestige rating occupation 0–100 “thermometer” scale, Canadian social survey conducted around time. type, type occupation, census, Census occupational code, used example. object regression analysis Prestige data (original purpose) predict occupational prestige variables data set. scatterplot matrix (using scatterplotMatrix() function car package) numeric variables data reveals distributions income women positively skewed, relationships among three predictors, predictors response (.e., prestige), nonlinear: Scatterplot matrix Prestige data. powerTransform() function car package transforms variables towards multivariate normality generalization Box Cox’s maximum-likelihood-like approach (Box & Cox, 1964). Several “families” power transformations can used, including original Box-Cox family, simple powers (roots), two adaptations Box-Cox family data may include negative values zeros: Box-Cox--negatives family Yeo-Johnson family; see Weisberg (2014, Chapter 8), Fox & Weisberg (2019, Chapter 3) details. women zero values, use Yeo-Johnson family: thus evidence desirability transforming income (\\(1/3\\) power) women (\\(0.16\\) power—close “0” power, .e., log transformation), education. Applying “rounded” power transformations makes predictors better-behaved: Scatterplot matrix Prestige data predictors transformed. Comparing MSE regressions original transformed predictors shows advantage latter: Similarly, component+residual plots two regressions, produced crPlots() function car package, suggest partial relationship prestige income nearly linear transformed data, transformation women fails capture appears slight quadratic partial relationship; partial relationship prestige education close linear regressions: Component+residual plots Prestige regression original predictors. Component+residual plots Prestige regression transformed predictors. transformed predictors towards multinormality, now consider whether ’s evidence transforming response (using powerTransform() Box Cox’s original method), discover ’s : selectTrans() function cv package automates process selecting predictor response transformations. function takes data set “working” model arguments, along candidate predictors response transformation, transformation family employ. predictors argument missing response transformed, response argument missing, supplied predictors transformed. default family transforming predictors \"bcPower\"—original Box-Cox family—default family.y transforming response; specify family=\"yjPower zeros women. selectTrans() returns result applying lack--fit criterion model selected transformation applied, default criterion=mse: selectTrans() also takes optional indices argument, making suitable computations subset data (.e., CV fold), hence use cvSelect() (see ?selectTrans details): results suggest predictive power transformed regression reliably greater untransformed regression (though case, cross-validated MSE considerably higher MSE computed whole data). Examining selected transformations fold reveals predictor education response prestige never transformed; \\(1/3\\) power selected income folds; transformation selected women varies narrowly across folds \\(0\\)th power (.e., log) \\(1/3\\) power.","code":"data(\"Prestige\", package=\"carData\") head(Prestige) #>                     education income women prestige census type #> gov.administrators      13.11  12351 11.16     68.8   1113 prof #> general.managers        12.26  25879  4.02     69.1   1130 prof #> accountants             12.77   9271 15.70     63.4   1171 prof #> purchasing.officers     11.42   8865  9.11     56.8   1175 prof #> chemists                14.62   8403 11.68     73.5   2111 prof #> physicists              15.64  11030  5.13     77.6   2113 prof summary(Prestige) #>    education         income          women          prestige        census     #>  Min.   : 6.38   Min.   :  611   Min.   : 0.00   Min.   :14.8   Min.   :1113   #>  1st Qu.: 8.45   1st Qu.: 4106   1st Qu.: 3.59   1st Qu.:35.2   1st Qu.:3120   #>  Median :10.54   Median : 5930   Median :13.60   Median :43.6   Median :5135   #>  Mean   :10.74   Mean   : 6798   Mean   :28.98   Mean   :46.8   Mean   :5402   #>  3rd Qu.:12.65   3rd Qu.: 8187   3rd Qu.:52.20   3rd Qu.:59.3   3rd Qu.:8312   #>  Max.   :15.97   Max.   :25879   Max.   :97.51   Max.   :87.2   Max.   :9517   #>    type    #>  bc  :44   #>  prof:31   #>  wc  :23   #>  NA's: 4   #>            #> library(\"car\") #> Loading required package: carData #>  #> Attaching package: 'car' #> The following object is masked from 'package:dplyr': #>  #>     recode scatterplotMatrix(~ prestige + income + education + women,                   data=Prestige, smooth=list(spread=FALSE)) trans <- powerTransform( cbind(income, education, women) ~ 1,                          data=Prestige, family=\"yjPower\") summary(trans) #> yjPower Transformations to Multinormality  #>           Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd #> income       0.2678        0.33       0.1051       0.4304 #> education    0.5162        1.00      -0.2822       1.3145 #> women        0.1630        0.16       0.0112       0.3149 #>  #>  Likelihood ratio test that all transformation parameters are equal to 0 #>                              LRT df    pval #> LR test, lambda = (0 0 0) 15.739  3 0.00128 P <- Prestige[, c(\"prestige\", \"income\", \"education\", \"women\")] (lambdas <- trans$roundlam) #>    income education     women  #>   0.33000   1.00000   0.16302 names(lambdas) <- c(\"income\", \"education\", \"women\") for (var in c(\"income\", \"education\", \"women\")){   P[, var] <- yjPower(P[, var], lambda=lambdas[var]) } summary(P) #>     prestige        income       education         women      #>  Min.   :14.8   Min.   :22.2   Min.   : 6.38   Min.   :0.00   #>  1st Qu.:35.2   1st Qu.:44.2   1st Qu.: 8.45   1st Qu.:1.73   #>  Median :43.6   Median :50.3   Median :10.54   Median :3.36   #>  Mean   :46.8   Mean   :50.8   Mean   :10.74   Mean   :3.50   #>  3rd Qu.:59.3   3rd Qu.:56.2   3rd Qu.:12.65   3rd Qu.:5.59   #>  Max.   :87.2   Max.   :83.6   Max.   :15.97   Max.   :6.83  scatterplotMatrix(~ prestige + income + education + women,                   data=P, smooth=list(spread=FALSE)) m.pres <- lm(prestige ~ income + education + women, data=Prestige) m.pres.trans <- lm(prestige ~ income + education + women, data=P) mse(Prestige$prestige, fitted(m.pres)) #> [1] 59.153 mse(P$prestige, fitted(m.pres.trans)) #> [1] 50.6 crPlots(m.pres) crPlots(m.pres.trans) summary(powerTransform(m.pres.trans)) #> bcPower Transformation to Normality  #>    Est Power Rounded Pwr Wald Lwr Bnd Wald Upr Bnd #> Y1    1.0194           1       0.6773       1.3615 #>  #> Likelihood ratio test that transformation parameter is equal to 0 #>  (log transformation) #>                          LRT df     pval #> LR test, lambda = (0) 32.217  1 1.38e-08 #>  #> Likelihood ratio test that no transformation is needed #>                            LRT df  pval #> LR test, lambda = (1) 0.012384  1 0.911 selectTrans(data=Prestige, model=m.pres,             predictors=c(\"income\", \"education\", \"women\"),             response=\"prestige\", family=\"yjPower\") #> [1] 50.6 cvs <- cvSelect(selectTrans, data=Prestige, model=m.pres, seed=1463,                 predictors=c(\"income\", \"education\", \"women\"),                 response=\"prestige\",                 family=\"yjPower\") #> R RNG seed set to 1463 cvs #> 10-Fold Cross Validation #> cross-validation criterion = 54.487 #> bias-adjusted cross-validation criterion = 54.308 #> full-sample criterion = 50.6  cv(m.pres, seed=1463) # untransformed model with same folds #> R RNG seed set to 1463 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 63.293 #> bias-adjusted cross-validation criterion = 63.073 #> full-sample criterion = 59.153  compareFolds(cvs) #>         lam.education lam.income lam.women lamda.y #> Fold 1          1.000      0.330     0.330       1 #> Fold 2          1.000      0.330     0.169       1 #> Fold 3          1.000      0.330     0.330       1 #> Fold 4          1.000      0.330     0.330       1 #> Fold 5          1.000      0.330     0.000       1 #> Fold 6          1.000      0.330     0.330       1 #> Fold 7          1.000      0.330     0.330       1 #> Fold 8          1.000      0.330     0.000       1 #> Fold 9          1.000      0.330     0.000       1 #> Fold 10         1.000      0.330     0.000       1"},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"parallel-computations","dir":"Articles","previous_headings":"","what":"Parallel computations","title":"Cross-validation of regression models","text":"CV functions cv package capable performing parallel computations setting ncores argument (specifying number computer cores used) number > 1 (default). Parallel computation can advantageous large problems, reducing execution time program. illustrate, let’s time model selection Mroz’s logistic regression, repeating computation performed previously parallel using 2 cores: small problem, parallel computation actually slower, overhead cost parallelization, can see produces result .","code":"system.time(m.mroz.sel.cv <- cvSelect(selectStepAIC, Mroz,                           seed=6681,                           criterion=BayesRule,                           model=m.mroz,                           AIC=FALSE)) #> R RNG seed set to 6681 #>    user  system elapsed  #>   0.455   0.000   0.455  system.time(m.mroz.sel.cv.p <- cvSelect(selectStepAIC, Mroz,                           seed=6681,                           criterion=BayesRule,                           model=m.mroz,                           AIC=FALSE,                           ncores=2)) #> R RNG seed set to 6681 #>    user  system elapsed  #>   0.064   0.004   1.785 all.equal(m.mroz.sel.cv, m.mroz.sel.cv.p) #> [1] TRUE"},{"path":[]},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"efficient-computations-for-linear-and-generalized-linear-models","dir":"Articles","previous_headings":"Computational notes","what":"Efficient computations for linear and generalized linear models","title":"Cross-validation of regression models","text":"straightforward way implement cross-validation R statistical modeling functions written canonical manner use update() refit model fold removed. approach taken default method cv(), appropriate cases independently sampled. Refitting model manner fold generally feasible number folds modest, can prohibitively costly leave-one-cross-validation number cases large. \"lm\" \"glm\" methods cv() take advantage computational efficiencies avoiding refitting model fold removed. Consider, particular, weighted linear model \\(\\mathbf{y}_{n \\times 1} = \\mathbf{X}_{n \\times p}\\boldsymbol{\\beta}_{p \\times 1} + \\boldsymbol{\\varepsilon}_{n \\times 1}\\), \\(\\boldsymbol{\\varepsilon} \\sim \\mathbf{N}_n \\left(\\mathbf{0}, \\sigma^2 \\mathbf{W}^{-1}_{n \\times n}\\right)\\). , \\(\\mathbf{y}\\) response vector, \\(\\mathbf{X}\\) model matrix, \\(\\boldsymbol{\\varepsilon}\\) error vector, \\(n\\) cases, \\(\\boldsymbol{\\beta}\\) vector \\(p\\) population regression coefficients. errors assumed multivariately normally distributed 0 means covariance matrix \\(\\sigma^2 \\mathbf{W}^{-1}\\), \\(\\mathbf{W} = \\mathrm{diag}(w_i)\\) diagonal matrix inverse-variance weights. linear model constant error variance, weight matrix taken \\(\\mathbf{W} = \\mathbf{}_n\\), order-\\(n\\) identity matrix. weighted-least-squares (WLS) estimator \\(\\boldsymbol{\\beta}\\) (see, e.g., Fox, 2016, sec. 12.2.2) 9 \\[ \\mathbf{b}_{\\mathrm{WLS}} = \\left( \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\right)^{-1}   \\mathbf{X}^T \\mathbf{W} \\mathbf{y} \\] Fitted values \\(\\widehat{\\mathbf{y}} = \\mathbf{X}\\mathbf{b}_{\\mathrm{WLS}}\\). LOO fitted value \\(\\)th case can efficiently computed \\(\\widehat{y}_{-} = y_i - e_i/(1 - h_i)\\) \\(h_i = \\mathbf{x}^T_i \\left( \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\right)^{-1} \\mathbf{x}_i\\) (-called “hatvalue”). , \\(\\mathbf{x}^T_i\\) \\(\\)th row \\(\\mathbf{X}\\), \\(\\mathbf{x}_i\\) \\(\\)th row written column vector. approach can break one hatvalues equal 1, case formula \\(\\widehat{y}_{-}\\) requires division 0. compute cross-validated fitted values folds contain one case, make use Woodbury matrix identify (\"Woodbury matrix identity\", 2023), \\[ \\left(\\mathbf{}_{m \\times m} + \\mathbf{U}_{m \\times k} \\mathbf{C}_{k \\times k} \\mathbf{V}_{k \\times m} \\right)^{-1} = \\mathbf{}^{-1} - \\mathbf{}^{-1}\\mathbf{U} \\left(\\mathbf{C}^{-1} + \\mathbf{VA}^{-1}\\mathbf{U} \\right)^{-1} \\mathbf{VA}^{-1} \\] \\(\\mathbf{}\\) nonsingular order-\\(n\\) matrix. apply result letting \\[\\begin{align*}     \\mathbf{} &= \\mathbf{X}^T \\mathbf{W} \\mathbf{X} \\\\     \\mathbf{U} &= \\mathbf{X}_\\mathbf{j}^T \\\\     \\mathbf{V} &= - \\mathbf{X}_\\mathbf{j} \\\\     \\mathbf{C} &= \\mathbf{W}_\\mathbf{j} \\\\ \\end{align*}\\] subscript \\(\\mathbf{j} = (i_{j1}, \\ldots, i_{jm})^T\\) represents vector indices cases \\(j\\)th fold, \\(j = 1, \\ldots, k\\). negative sign \\(\\mathbf{V} = - \\mathbf{X}_\\mathbf{j}\\) reflects removal, rather addition, cases \\(\\mathbf{j}\\). Applying Woodbury identity isn’t quite fast using hatvalues, generally much faster refitting model. disadvantage Woodbury identity, however, entails explicit matrix inversion thus may numerically unstable. inverse \\(\\mathbf{} = \\mathbf{X}^T \\mathbf{W} \\mathbf{X}\\) available directly \"lm\" object, second term right-hand side Woodbury identity requires matrix inversion fold deleted. (contrast, inverse \\(\\mathbf{C} = \\mathbf{W}_\\mathbf{j}\\) straightforward \\(\\mathbf{W}\\) diagonal.) Woodbury identity also requires model matrix full rank. impose restriction code removing redundant regressors model matrix cases, doesn’t preclude rank deficiency surfacing fold removed. Rank deficiency \\(\\mathbf{X}\\) doesn’t disqualify cross-validation need fitted values estimated model. glm() computes maximum-likelihood estimates generalized linear model iterated weighted least squares (see, e.g., Fox & Weisberg, 2019, sec. 6.12). last iteration therefore just WLS fit “working response” model matrix using “working weights.” working weights working response convergence available information object returned glm(). treat re-estimation model case cases deleted WLS problem, using hatvalues Woodbury matrix identity. resulting fitted values deleted fold aren’t exact—, except Gaussian family, result isn’t identical obtain literally refitting model—(limited) experience, approximation good, especially LOO CV, tempted use . Nevertheless, results approximate, default \"glm\" cv() method perform exact computation, entails refitting model fold omitted.","code":""},{"path":"https://gmonette.github.io/cv/articles/cv.html","id":"computation-of-the-bias-corrected-cv-criterion","dir":"Articles","previous_headings":"Computational notes","what":"Computation of the bias-corrected CV criterion","title":"Cross-validation of regression models","text":"Let \\(\\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}})\\) represent cross-validation cost criterion, mean-squared error, computed \\(n\\) values response \\(\\mathbf{y}\\) based fitted values \\(\\widehat{\\mathbf{y}}\\) model fit data. divide \\(n\\) cases \\(k\\) folds approximately \\(n_j \\approx n/k\\) cases , \\(n = \\sum n_j\\). , let \\(\\mathbf{j}\\) denote indices cases \\(j\\)th fold. Now define \\(\\mathrm{CV}_j = \\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}}^{(j)})\\) \\(\\mathrm{CV}_j^{(j)} = \\mathrm{CV}(\\mathbf{y}_{\\mathbf{j}}, \\widehat{\\mathbf{y}}^{(j)}_{\\mathbf{j}})\\). superscript \\((j)\\) \\(\\widehat{\\mathbf{y}}^{(j)}\\) \\(\\widehat{\\mathbf{y}}^{(j)}_{\\mathbf{j}}\\) represents fitted values computed model fold \\(j\\) omitted, respectively cases (.e., \\(\\mathrm{CV}_j\\)) cases \\(j\\)th fold (\\(\\mathrm{CV}_j^{(j)}\\)). cross-validation criterion weighted average \\(\\mathrm{CV}_j^{(j)}\\) \\[ \\mathrm{CV} = \\frac{1}{n} \\sum_{j = 1}^{k} n_j \\mathrm{CV}_j^{(j)} \\] Following Davison & Hinkley (1997, pp. 293–295), bias-adjusted cross-validation criterion \\[ \\mathrm{CV}_{\\mathrm{adj}} = \\mathrm{CV} + \\mathrm{CV}(\\mathbf{y}, \\widehat{\\mathbf{y}}) - \\frac{1}{n} \\sum_{j=1}^{k} n_j \\mathrm{CV}_j \\]","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"John Fox. Author. Georges Monette. Author, maintainer.","code":""},{"path":"https://gmonette.github.io/cv/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Fox J, Monette G (2023). cv: Cross-Validation Regression Models. R package version 0.1.0, https://gmonette.github.io/cv/.","code":"@Manual{,   title = {cv: Cross-Validation of Regression Models},   author = {John Fox and Georges Monette},   year = {2023},   note = {R package version 0.1.0},   url = {https://gmonette.github.io/cv/}, }"},{"path":"https://gmonette.github.io/cv/index.html","id":"cv-package-for-r-various-functions-for-cross-validation-of-regression-models-","dir":"","previous_headings":"","what":"Cross-Validation of Regression Models","title":"Cross-Validation of Regression Models","text":"functions supplied package: cv() generic function default method computationally efficient \"lm\" \"glm\" methods, along method list competing models. also experimental \"merMod\", \"lme\", \"glmmTMB\" methods mixed-effects models. cv() supports parallel computations. mse() (mean-squared error) BayesRule() cross-validation criteria (“cost functions”), suitable use cv(). cvSelect() cross-validates selection procedure regression model. cvSelect() also supports parallel computations. selectStepAIC() model-selection procedure, suitable use cvSelect(), based stepAIC() function MASS package. selectTrans() procedure selecting predictor response transformations regression, also suitable use cvSelect(), based powerTransform() function car package. additional information using cv package, see “Cross-validation regression models” vignette (vignette(\"cv\", package=\"cv\")). cv package designed extensible classes regression models model-selection procedures; details, see “Extending cv package” vignette (vignette(\"cv-extend\", package=\"cv\")). time-cv package resides GitHub, anticipate eventually submitting CRAN. install package:","code":"if (!require(remotes)) install.packages(\"remotes\") remotes::install_github(\"gmonette/cv\", build_vignettes=TRUE,   dependencies=TRUE)"},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Cost Functions for Fitted Regression Models — mse","title":"Cost Functions for Fitted Regression Models — mse","text":"Compute cost functions (cross-validation criteria) fitted regression models.","code":""},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cost Functions for Fitted Regression Models — mse","text":"","code":"mse(y, yhat)  BayesRule(y, yhat)  BayesRule2(y, yhat)"},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cost Functions for Fitted Regression Models — mse","text":"y response yhat fitted value","code":""},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cost Functions for Fitted Regression Models — mse","text":"Two cost functions (cross-validation criteria) provided: (1) mse() returns mean-squared error prediction numeric response variable y predictions yhat. (2) BayesRule() BayesRule2() report proportion correct predictions dichotomous response variable y, assumed coded 0 1. yhat values predicted probabilities rounded 0 1. distinction BayesRule() BayesRule2() former checks y values either 0 1 yhat values 0 1, latter therefore faster.","code":""},{"path":"https://gmonette.github.io/cv/reference/cost-functions.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cost Functions for Fitted Regression Models — mse","text":"mse(): Mean-square error BayesRule(): Bayes Rule binary response BayesRule2(): Bayes rule binary response (without bounds checking)","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validate Regression Models — cv","title":"Cross-Validate Regression Models — cv","text":"parallelized generic k-fold (including n-fold, .e., leave-one-) cross-validation function, default method, specific methods linear generalized-linear models can much computationally efficient.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validate Regression Models — cv","text":"","code":"cv(model, data, criterion, k, reps = 1, seed, ...)  # S3 method for default cv(   model,   data = insight::get_data(model),   criterion = mse,   k = 10,   reps = 1,   seed,   ncores = 1,   method = NULL,   type = \"response\",   ... )  # S3 method for cv print(x, digits = getOption(\"digits\"), ...)  # S3 method for cvList print(x, ...)  # S3 method for lm cv(   model,   data = insight::get_data(model),   criterion = mse,   k = 10,   reps = 1,   seed,   method = c(\"auto\", \"hatvalues\", \"Woodbury\", \"naive\"),   ncores = 1,   ... )  # S3 method for glm cv(   model,   data = insight::get_data(model),   criterion = mse,   k = 10,   reps = 1,   seed,   method = c(\"exact\", \"hatvalues\", \"Woodbury\"),   ncores = 1,   ... )"},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validate Regression Models — cv","text":"model regression model object (see Details). data data frame model fit (usually necessary). criterion cross-validation criterion (\"cost\") function form f(y, yhat) y observed values response yhat predicted values; default mse (mean-squared error). k perform k-fold cross-validation (default 10); k may number \"loo\" \"n\" n-fold (leave-one-) cross-validation. reps number times replicate k-fold CV (default 1) seed R's random number generator; optional, supplied random seed selected saved; needed n-fold cross-validation ... match generic; passed predict() default method. ncores number cores use parallel computations (default 1, .e., computations done parallel) method computational method apply linear (.e. \"lm\") model generalized linear (.e., \"glm\") model. See Details explanation available options. type default method, value passed type argument predict(); default type=\"response\", appropriate, e.g., \"glm\" model may recognized ignored predict() methods model classes. x cv object printed digits significant digits printing, default taken \"digits\" option","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validate Regression Models — cv","text":"object class \"cv\", averaged CV criterion (\"CV crit\"), adjusted average CV criterion (\"adj CV crit\"), criterion model applied full data (\"full crit\"), number folds (\"k\"), seed R's random-number generator (\"seed\"). methods may return subset components may add additional information. reps > 1, object class \"cvList\" returned, literally list \"cv\" objects.","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validate Regression Models — cv","text":"default method uses update() refit model fold, work appropriate update() predict() methods default method getResponse() works getResponse() method supplied. \"lm\" \"glm\" methods can use much faster computational algorithms, selected method argument. linear-model method accommodates weighted linear models. classes models, leave-one-(n-fold) case, fitted values folds can computed hat-values via method=\"hatvalues\" without refitting model; GLMs, method approximate, LMs exact. classes models, one case omitted fold, fitted values may obtained without refitting model exploiting Woodbury matrix identity via method=\"Woodbury\". hatvalues, method exact LMs approximate GLMs. default linear models method=\"auto\", equivalent method=\"hatvalues\" n-fold cross-validation method=\"Woodbury\" otherwise; method=\"naive\" refits model via update() generally much slower. default generalized linear models method=\"exact\", employs update(). additional details, see \"Cross-validation regression models\" vignette (vignette(\"cv\", package=\"cv\")). cv() designed extensible classes regression models; see \"Extending cv package\" vignette (vignette(\"cv-extend\", package=\"cv\")).","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"methods-by-class-","dir":"Reference","previous_headings":"","what":"Methods (by class)","title":"Cross-Validate Regression Models — cv","text":"cv(default): default method cv(lm): \"lm\" method cv(glm): \"glm\" method","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"methods-by-generic-","dir":"Reference","previous_headings":"","what":"Methods (by generic)","title":"Cross-Validate Regression Models — cv","text":"print(cv): print() method","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cross-Validate Regression Models — cv","text":"print(cvList): print() method","code":""},{"path":"https://gmonette.github.io/cv/reference/cv.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validate Regression Models — cv","text":"","code":"data(\"Auto\", package=\"ISLR2\") m.auto <- lm(mpg ~ horsepower, data=Auto) cv(m.auto,  k=\"loo\") #> n-Fold Cross Validation #> method: hatvalues #> criterion: mse #> cross-validation criterion = 24.23151 cv(m.auto, seed=1234) #> R RNG seed set to 1234 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.3794 #> bias-adjusted cross-validation criterion = 24.35646 #> full-sample criterion = 23.94366  cv(m.auto, seed=1234, reps=3) #> R RNG seed set to 1234 #> R RNG seed set to 469908 #> R RNG seed set to 267 #>  #> Replicate 1: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.28572 #> bias-adjusted cross-validation criterion = 24.26775 #> full-sample criterion = 23.94366  #>  #> Replicate 2: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.30002 #> bias-adjusted cross-validation criterion = 24.28129 #> full-sample criterion = 23.94366  #>  #> Replicate 3: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.3794 #> bias-adjusted cross-validation criterion = 24.35646 #> full-sample criterion = 23.94366  #>  #> Average: #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 24.31271 (0.04496328) #> bias-adjusted cross-validation criterion = 24.29331 (0.0425791) #> full-sample criterion = 23.94366   data(\"Mroz\", package=\"carData\") m.mroz <- glm(lfp ~ ., data=Mroz, family=binomial) cv(m.mroz, criterion=BayesRule, seed=123) #> R RNG seed set to 123 #> 10-Fold Cross Validation #> method: exact #> criterion: BayesRule #> cross-validation criterion = 0.3320053 #> bias-adjusted cross-validation criterion = 0.3260248 #> full-sample criterion = 0.3067729"},{"path":"https://gmonette.github.io/cv/reference/cvMixed.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validate Mixed-Effects Model — cvMixed","title":"Cross-Validate Mixed-Effects Model — cvMixed","text":"cv() methods models class \"merMod\", fit lmer() glmer() functions lme4 package; models class \"lme\" fit lme() function nlme package; models class \"glmmTMB\" fit glmmTMB() function glmmTMB package. implementations regarded experimental. cvMixed() function meant called cv() methods mixed-effect models directly user.","code":""},{"path":"https://gmonette.github.io/cv/reference/cvMixed.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validate Mixed-Effects Model — cvMixed","text":"","code":"cvMixed(   model,   data = insight::get_data(model),   criterion = mse,   k,   reps = 1,   seed,   ncores = 1,   clusterVariables,   predict.clusters.args = list(object = model, newdata = data),   predict.cases.args = list(object = model, newdata = data),   ... )  # S3 method for merMod cv(   model,   data = insight::get_data(model),   criterion = mse,   k,   reps = 1,   seed,   ncores = 1,   clusterVariables,   ... )  # S3 method for lme cv(   model,   data = insight::get_data(model),   criterion = mse,   k,   reps = 1,   seed,   ncores = 1,   clusterVariables,   ... )  # S3 method for glmmTMB cv(   model,   data = insight::get_data(model),   criterion = mse,   k,   reps = 1,   seed,   ncores = 1,   clusterVariables,   ... )"},{"path":"https://gmonette.github.io/cv/reference/cvMixed.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validate Mixed-Effects Model — cvMixed","text":"model regression model object class \"merMod\". data data frame model fit (usually necessary) criterion cross-validation criterion function form f(y, yhat) y observed values response yhat predicted values; default mse (mean-squared error) k perform k-fold cross-validation; k may number \"loo\" \"n\" n-fold (leave-one-) cross-validation; default 10 cross-validating individual cases \"loo\" cross-validating clusters. reps number times replicate k-fold CV (default 1) seed R's random number generator; optional, supplied random seed selected saved; needed n-fold cross-validation ncores number cores use parallel computations (default 1, .e., computations done parallel) clusterVariables character vector names variables defining clusters mixed model nested random effects; missing, cross-validation performed individual cases rather clusters predict.clusters.args list arguments used predict whole data set mixed model performing CV clusters; first two elements model newdata; see \"Extending cv package\" vignette (vignette(\"cv-extend\", package=\"cv\")). predict.cases.args list arguments used predict whole data set mixed model performing CV cases; first two elements model newdata; see \"Extending cv package\" vignette (vignette(\"cv-extend\", package=\"cv\")). ... cv() methods, match generic, cvMixed(), arguments passed update().","code":""},{"path":"https://gmonette.github.io/cv/reference/cvMixed.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validate Mixed-Effects Model — cvMixed","text":"mixed-effects models, cross-validation can done \"clusters\" individual observations. former, predictions based fixed effects; latter, predictions include random effects. mixed models fully nested random effects supported.","code":""},{"path":"https://gmonette.github.io/cv/reference/cvMixed.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cross-Validate Mixed-Effects Model — cvMixed","text":"cvMixed(): called directly cv(merMod): cv() method cv(lme): cv() method cv(glmmTMB): cv() method","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/reference/cvMixed.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validate Mixed-Effects Model — cvMixed","text":"","code":"library(\"lme4\") #> Loading required package: Matrix # from ?lmer: (fm1 <- lmer(Reaction ~ Days + (Days | Subject), sleepstudy)) #> Linear mixed model fit by REML ['lmerMod'] #> Formula: Reaction ~ Days + (Days | Subject) #>    Data: sleepstudy #> REML criterion at convergence: 1743.628 #> Random effects: #>  Groups   Name        Std.Dev. Corr #>  Subject  (Intercept) 24.741        #>           Days         5.922   0.07 #>  Residual             25.592        #> Number of obs: 180, groups:  Subject, 18 #> Fixed Effects: #> (Intercept)         Days   #>      251.41        10.47   cv(fm1, clusterVariables=\"Subject\") # LOO CV of clusters #> n-Fold Cross Validation based on 18 {Subject} clusters #> cross-validation criterion = 2460.604 #> bias-adjusted cross-validation criterion = 2454.627 #> full-sample criterion = 2251.398  cv(fm1, seed=447) # 10-fold CV of cases #> R RNG seed set to 447 #> 10-Fold Cross Validation #> cross-validation criterion = 869.533 #> bias-adjusted cross-validation criterion = 847.5586 #> full-sample criterion = 549.342  cv(fm1, clusterVariables=\"Subject\", k=5,    seed=834, reps=3) # 5-fold CV of clusters, repeated 3 times #> R RNG seed set to 834 #> R RNG seed set to 448690 #> R RNG seed set to 916534 #>  #> Replicate 1: #> 5-Fold Cross Validation based on 18 {Subject} clusters #> cross-validation criterion = 2458.192 #> bias-adjusted cross-validation criterion = 2434.117 #> full-sample criterion = 2251.398  #>  #> Replicate 2: #> 5-Fold Cross Validation based on 18 {Subject} clusters #> cross-validation criterion = 2511.651 #> bias-adjusted cross-validation criterion = 2479.989 #> full-sample criterion = 2251.398  #>  #> Replicate 3: #> 5-Fold Cross Validation based on 18 {Subject} clusters #> cross-validation criterion = 2403.755 #> bias-adjusted cross-validation criterion = 2387.049 #> full-sample criterion = 2251.398  #>  #> Average: #> 5-Fold Cross Validation based on 18 {Subject} clusters #> cross-validation criterion = 2457.947 (44.04918) #> bias-adjusted cross-validation criterion = 2433.818 (37.94423) #> full-sample criterion = 2251.398   library(nlme) #>  #> Attaching package: ‘nlme’ #> The following object is masked from ‘package:lme4’: #>  #>     lmList #> The following object is masked from ‘package:cv’: #>  #>     getResponse # from ?lme (fm2 <- lme(distance ~ age + Sex, data = Orthodont,             random = ~ 1)) #> Linear mixed-effects model fit by REML #>   Data: Orthodont  #>   Log-restricted-likelihood: -218.7563 #>   Fixed: distance ~ age + Sex  #> (Intercept)         age   SexFemale  #>  17.7067130   0.6601852  -2.3210227  #>  #> Random effects: #>  Formula: ~1 | Subject #>         (Intercept) Residual #> StdDev:    1.807425 1.431592 #>  #> Number of Observations: 108 #> Number of Groups: 27  cv(fm2) # LOO CV of cases #> R RNG seed set to 765199 #> 10-Fold Cross Validation #> cross-validation criterion = 2.666103 #> bias-adjusted cross-validation criterion = 2.589541 #> full-sample criterion = 1.582435  cv(fm2, clusterVariables=\"Subject\", k=5, seed=321) # 5-fold CV of clusters #> R RNG seed set to 321 #> 5-Fold Cross Validation based on 27 {Subject} clusters #> cross-validation criterion = 5.875411 #> bias-adjusted cross-validation criterion = 5.780695 #> full-sample criterion = 5.017326"},{"path":"https://gmonette.github.io/cv/reference/cvSelect.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validate a Model-Selection Procedure — cvSelect","title":"Cross-Validate a Model-Selection Procedure — cvSelect","text":"generic function cross-validate model-selection procedure, along procedure applies stepAIC() model-selection function MASS package, procedure selecting predictor response transformations regression, uses powerTransform() function car package.","code":""},{"path":"https://gmonette.github.io/cv/reference/cvSelect.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validate a Model-Selection Procedure — cvSelect","text":"","code":"cvSelect(   procedure,   data,   k = 10,   reps = 1,   save.coef = k <= 10,   seed,   ncores = 1,   ... )  selectStepAIC(   data,   indices,   model,   criterion = mse,   AIC = TRUE,   save.coef = TRUE,   ... )  selectTrans(   data,   indices,   save.coef = TRUE,   model,   criterion = mse,   predictors,   response,   family = c(\"bcPower\", \"bcnPower\", \"yjPower\", \"basicPower\"),   family.y = c(\"bcPower\", \"bcnPower\", \"yjPower\", \"basicPower\"),   rounded = TRUE,   ... )  compareFolds(object, digits = 3, ...)"},{"path":"https://gmonette.github.io/cv/reference/cvSelect.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validate a Model-Selection Procedure — cvSelect","text":"procedure model-selection procedure function (see Details). data full data frame model selection. k perform k-fold cross-validation (default 10); k may number \"loo\" \"n\" n-fold (leave-one-) cross-validation. reps number times replicate k-fold CV (default 1) save.coef save coefficients selected models? (default TRUE k 10 smaller, FALSE otherwise) seed R's random number generator; used n-fold cross-validation. ncores number cores use parallel computations (default 1, .e., computations done parallel) ... arguments passed procedure(). indices indices cases data defining current fold. model regression model object fit data. criterion CV criterion function. AIC TRUE (default) use AIC model-selection criterion; FALSE, use BIC. k argument stepAIC() set accordingly (note distinct number folds k). predictors character vector names predictors model transform; missing, predictors transformed. response name response variable; missing, response transformed. family transformation family predictors, one \"bcPower\", \"bcnPower\", \"yjPower\", \"basicPower\", \"bcPower\" default. names transformation functions car package; see bcPower family.y transformation family response, \"bcPower\" default. rounded TRUE (default) use nicely rounded versions estimated transformation parameters (see bcPower). object object class \"cvSelect\". digits significant digits printing coefficients, (default 3).","code":""},{"path":"https://gmonette.github.io/cv/reference/cvSelect.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validate a Model-Selection Procedure — cvSelect","text":"object class \"cvSelect\", inheriting class \"cv\", averaged CV criterion (\"CV crit\"), adjusted average CV criterion (\"adj CV crit\"), criterion model applied full data (\"full crit\"), number folds (\"k\"), seed R's random-number generator (\"seed\"), (optionally) list coefficients (, case selectTrans(), estimated transformation parameters) selected models fold (\"coefficients\"). reps > 1, object class c(\"cvSelectList\", \"cvList\") returned, literally list c(\"cvSelect\", \"cv\") objects.","code":""},{"path":"https://gmonette.github.io/cv/reference/cvSelect.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Cross-Validate a Model-Selection Procedure — cvSelect","text":"model-selection function supplied procedure argument cvSelect() accept following arguments: data set data argument cvSelect(). indices indices rows data defining current fold; missing, model-selection procedure applied full data. arguments passed via ... cvSelect(). procedure() return two-element vector result applying cross-validation criterion cases current fold model deleting fold, cases model deleting current fold. indices argument missing, procedure() returns cross-validation criterion cases based model fit cases. examples model-selection functions procedure argument, see code selectStepAIC() selectTrans(). additional information, see \"Cross-validation regression models\" vignette (vignette(\"cv\", package=\"cv\")) \"Extending cv package\" vignette (vignette(\"cv-extend\", package=\"cv\")).","code":""},{"path":"https://gmonette.github.io/cv/reference/cvSelect.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cross-Validate a Model-Selection Procedure — cvSelect","text":"cvSelect(): apply cross-validation model-selection procedure. selectStepAIC(): select model using stepAIC() function MASS package. selectTrans(): select transformations predictors response. compareFolds(): print coefficients selected models several folds.","code":""},{"path":[]},{"path":"https://gmonette.github.io/cv/reference/cvSelect.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validate a Model-Selection Procedure — cvSelect","text":"","code":"data(\"Auto\", package=\"ISLR2\") m.auto <- lm(mpg ~ . - name - origin, data=Auto) cvSelect(selectStepAIC, Auto, seed=123, model=m.auto) #> R RNG seed set to 123 #> 10-Fold Cross Validation #> cross-validation criterion = 11.87795 #> bias-adjusted cross-validation criterion = 11.8753 #> full-sample criterion = 11.65549  cvSelect(selectStepAIC, Auto, seed=123, model=m.auto,          AIC=FALSE, k=5, reps=3) # via BIC #> R RNG seed set to 123 #> R RNG seed set to 319729 #> R RNG seed set to 889422 #>  #> Replicate 1: #> 5-Fold Cross Validation #> cross-validation criterion = 11.91891 #> bias-adjusted cross-validation criterion = 11.8895 #> full-sample criterion = 11.65549  #>  #> Replicate 2: #> 5-Fold Cross Validation #> cross-validation criterion = 11.80601 #> bias-adjusted cross-validation criterion = 11.78946 #> full-sample criterion = 11.65549  #>  #> Replicate 3: #> 5-Fold Cross Validation #> cross-validation criterion = 11.81343 #> bias-adjusted cross-validation criterion = 11.79576 #> full-sample criterion = 11.65549  #>  #> Average: #> 5-Fold Cross Validation #> cross-validation criterion = 11.86432 (0.06311657) #> bias-adjusted cross-validation criterion = 11.84105 (0.05600268) #> full-sample criterion = 11.65549   data(\"Prestige\", package=\"carData\") m.pres <- lm(prestige ~ income + education + women,              data=Prestige) cvt <- cvSelect(selectTrans, data=Prestige, model=m.pres, seed=123,                 predictors=c(\"income\", \"education\", \"women\"),                 response=\"prestige\", family=\"yjPower\") #> R RNG seed set to 123 cvt #> 10-Fold Cross Validation #> cross-validation criterion = 58.68193 #> bias-adjusted cross-validation criterion = 58.26258 #> full-sample criterion = 50.60016  compareFolds(cvt) #>         lam.education lam.income lam.women lamda.y #> Fold 1          1.000      0.330     0.330       1 #> Fold 2          1.000      0.330     0.330       1 #> Fold 3          1.000      0.330     0.000       1 #> Fold 4          1.000      0.330     0.330       1 #> Fold 5          1.000      0.330     0.000       1 #> Fold 6          1.000      0.330     0.000       1 #> Fold 7          1.000      0.000     0.000       1 #> Fold 8          1.000      0.330     0.000       1 #> Fold 9          1.000      0.500     0.330       1 #> Fold 10         1.000      0.330     0.159       1 cv(m.pres, seed=123) #> R RNG seed set to 123 #> 10-Fold Cross Validation #> method: Woodbury #> criterion: mse #> cross-validation criterion = 66.91929 #> bias-adjusted cross-validation criterion = 66.45514 #> full-sample criterion = 59.15265"},{"path":"https://gmonette.github.io/cv/reference/getResponse.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Response Variable — getResponse","title":"Extract Response Variable — getResponse","text":"Generic function extract response variable fitted model.","code":""},{"path":"https://gmonette.github.io/cv/reference/getResponse.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Response Variable — getResponse","text":"","code":"getResponse(model, ...)  # S3 method for default getResponse(model, ...)  # S3 method for merMod getResponse(model, ...)  # S3 method for lme getResponse(model, ...)  # S3 method for glmmTMB getResponse(model, ...)"},{"path":"https://gmonette.github.io/cv/reference/getResponse.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Response Variable — getResponse","text":"model fitted model ... additional parameters specific methods","code":""},{"path":"https://gmonette.github.io/cv/reference/getResponse.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Response Variable — getResponse","text":"numeric vector containing values response variable","code":""},{"path":"https://gmonette.github.io/cv/reference/getResponse.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extract Response Variable — getResponse","text":"supplied default method returns model$y component model object, , model S4 object, result returned get_response() function insight package. result NULL, result model.response(model.frame(model)) returned, checking case whether result numeric vector. also \"lme\" method, \"merMod\" method converts factor responses numeric 0/1 responses, appropriate generalized linear mixed models binary response.","code":""},{"path":"https://gmonette.github.io/cv/reference/getResponse.html","id":"methods-by-class-","dir":"Reference","previous_headings":"","what":"Methods (by class)","title":"Extract Response Variable — getResponse","text":"getResponse(default): default method getResponse(merMod): merMod method getResponse(lme): merMod method getResponse(glmmTMB): glmmTMB method","code":""},{"path":"https://gmonette.github.io/cv/reference/getResponse.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Extract Response Variable — getResponse","text":"","code":"fit <- lm(mpg ~ gear, mtcars)     getResponse(fit) #> Error in UseMethod(\"getResponse\"): no applicable method for 'getResponse' applied to an object of class \"lm\""},{"path":"https://gmonette.github.io/cv/reference/models.html","id":null,"dir":"Reference","previous_headings":"","what":"Cross-Validate Several Models Fit to the Same Data — models","title":"Cross-Validate Several Models Fit to the Same Data — models","text":"cv() method object class  \"modlist\", created models() function. cv() method simplifies process cross-validating several models set folds. models() performs \"sanity\" checks, warning models different classes, reporting error fit apparently different data sets different response variables.","code":""},{"path":"https://gmonette.github.io/cv/reference/models.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Cross-Validate Several Models Fit to the Same Data — models","text":"","code":"models(...)  # S3 method for modList cv(model, data, criterion = mse, k, reps = 1, seed, quietly = TRUE, ...)  # S3 method for cvModList print(x, ...)  # S3 method for cvModList plot(   x,   y,   xlab = \"\",   ylab,   main,   axis.args = list(labels = names(x), las = 3L),   col = palette()[2L],   lwd = 2,   ... )"},{"path":"https://gmonette.github.io/cv/reference/models.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Cross-Validate Several Models Fit to the Same Data — models","text":"... cv(), additional arguments passed cv() method applied model. models(), two competing models fit data; several models may named. print() method, arguments passed print() method individual model cross-validations. plot(), method, arguments passed base plot() function. model list regression model objects, created models(). data (required) data set models fit. criterion CV criterion (cost) function, defaults mse. k number CV folds; may omitted, case value depend default cv() method invoked individual models. reps number replications CV model. seed (optional) seed R's pseudo-random-number generator, used create set CV folds models; omitted, seed randomly generated saved. quietly TRUE (default), simple messages (example value random-number generator seed set), warnings errors, suppressed. x object class \"cvModList\" printed plotted. y name element \"cv\" object plotted; defaults \"adj CV crit\", exists, \"CV crit\". xlab label x-axis (defaults blank). ylab label y-axis (missing, label constructed). main main title graph (missing, label constructed). axis.args list arguments axis() function, used draw horizontal axis. addition axis arguments given explicitly, side=1 (horizontal axis) =seq(along=x) (.e., 1 number models) used modified. col color line points, defaults second element color palette; see palette(). lwd line width line (defaults 2).","code":""},{"path":"https://gmonette.github.io/cv/reference/models.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Cross-Validate Several Models Fit to the Same Data — models","text":"models() returns \"modList\" object, cv() method returns \"cvModList\" object.","code":""},{"path":"https://gmonette.github.io/cv/reference/models.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Cross-Validate Several Models Fit to the Same Data — models","text":"models(): create list models cv(modList): cv() method \"modList\" objects print(cvModList): print() method \"cvModList\" objects plot(cvModList): plot() method \"cvModList\" objects","code":""},{"path":"https://gmonette.github.io/cv/reference/models.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Cross-Validate Several Models Fit to the Same Data — models","text":"","code":"data(\"Duncan\", package=\"carData\") m1 <- lm(prestige ~ income + education, data=Duncan) m2 <- lm(prestige ~ income + education + type, data=Duncan) m3 <- lm(prestige ~ (income + education)*type, data=Duncan) (cv.models <- cv(models(m1=m1, m2=m2, m3=m3),                  data=Duncan, seed=7949, reps=5)) #>  #> Model m1 (averaged across 5 replications): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 195.0866 (2.307011) #> bias-adjusted cross-validation criterion = 193.5434 (2.195082) #> full-sample criterion = 166.8155  #>  #> Model m2 (averaged across 5 replications): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 111.7281 (4.908514) #> bias-adjusted cross-validation criterion = 110.1813 (4.623137) #> full-sample criterion = 84.39899  #>  #> Model m3 (averaged across 5 replications): #> 10-Fold Cross Validation #> method: Woodbury #> cross-validation criterion = 142.3619 (5.682756) #> bias-adjusted cross-validation criterion = 137.932 (5.264929) #> full-sample criterion = 74.45878  plot(cv.models)"},{"path":"https://gmonette.github.io/cv/news/index.html","id":"cv-010","dir":"Changelog","previous_headings":"","what":"cv 0.1.0","title":"cv 0.1.0","text":"Initial version.","code":""}]
