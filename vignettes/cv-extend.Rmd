---
title: "Extending the cv package"
author: "John Fox and Georges Monette"
date: "`r Sys.Date()`"
package: cv
output: 
  rmarkdown::html_vignette:
  fig_caption: yes
bibliography: ["cv.bib"]
csl: apa.csl
vignette: >
  %\VignetteIndexEntry{Extending the cv package}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  message = TRUE,
  warning = TRUE,
  fig.align = "center",
  fig.height = 6,
  fig.width = 7,
  fig.path = "fig/",
  dev = "png",
  comment = "#>" #,
  # eval = nzchar(Sys.getenv("REBUILD_VIGNETTES"))
)

# save some typing
knitr::set_alias(w = "fig.width",
                 h = "fig.height",
                 cap = "fig.cap")

# colorize text: use inline as `r colorize(text, color)`
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}


.opts <- options(digits = 5)
```

The **cv** package is designed to be extensible in several directions. In this vignette, we discuss three kinds of extensions, ordered by increasing general complexity: (1) adding a cross-validation cost criterion; (2) adding a model class that's not directly accommodated by the `cv()` default method or by another directly inherited method; and (3) adding a new model-selection procedure suitable for use with `selectModel()`.


## Adding a cost criterion

A cost criterion suitable for use with `cv()` or `cvSelect()` should take two arguments, `y` (the observed response vector) and `yhat` (a vector of fitted or predicted response values), and return a numeric index of lack of fit. The **cv** package supplies two such criteria: `mse(y, yhat)`, which returns the mean-squared prediction error for a numeric response; and `BayesRule(y, yhat)` (and its non-error-checking version, `BayesRule2(y, yhat))`, suitable for use with a binary regression model, where `y` is the binary response coded `0` for a "failure" or `1` for a "success," where `yhat` is the predicted probability of success, and which returns the proportion of *incorrectly* classified cases.

To illustrate using a different prediction cost criterion, we'll base a cost criterion on the the area under the receiver operating characteristic ("ROC") curve for a logistic regression. The ROC curve is a graphical representation of the classification power of a binary regression model, and the area under the ROC curve ("AOC"), which varies from 0 to 1, is a common summary measure based on the ROC [see @Wikipedia-ROC:2023]. The **Metrics** package [@HamnerFrasco:2018] includes a variety of measures useful for model selection, including an `auc()` function. We convert the AUC into a cost measure by taking its complement:

```{r AUCcomp}
AUCcomp <- function(y, yhat) 1 - Metrics::auc(y, yhat)
```

We then apply `AUCcomp()` to the a logistic regression discussed in the main **cv** package vignette, which we reproduce here, using the `Mroz` data frame from the **carData** package [@FoxWeisberg:2019]:
```{r Mroz-regression}
data("Mroz", package="carData")
m.mroz <- glm(lfp ~ ., data=Mroz, family=binomial)
summary(m.mroz)

AUCcomp(with(Mroz, as.numeric(lfp == "yes")), fitted(m.mroz))
```
Cross-validating this cost measure is straightforward:
```{r Mroz-CV-ROC}
library("cv")
cv(m.mroz, criterion=AUCcomp, seed=3639)
```
As expected, the cross-validated complement to the AUC is somewhat less optimistic that the criterion computed from the model fit to the whole data set.

## Adding a model class not covered by the default `cv()` method

Suppose that we want to cross-validate a multinomial logistic regression model fit by the `multinom()` function in the **nnet** package [@VenablesRipley:2002]. We borrow an example from @Fox:2016 [Sec. 14.2.1], with data from the British Election Panel Study on vote choice in the 2001 British election. Data for the example are in the `BEPS` data frame in the **carData** package:
```{r BEPS-data}
data("BEPS", package="carData")
head(BEPS)
```
The polytomous (multi-category) response variable is `vote`, a factor with levels `"Conservative"`, `"Labour"`, and `"Liberal Democrat"`. The predictors of `vote` are:

* `age`, in years;
* `econ.cond.national` and `econ.cond.household`, the respondent's ratings of the state of the economy, on 1 to 5 scales.
* `Blair`, `Hague`, and `Kennedy`, ratings of the leaders of the Labour, Conservative, and Liberal Democratic parties, on 1 to 5 scales.
* `Europe`, an 11-point scale on attitude towards European integration, with high scores representing "Euro-skepticism."
* `political.knowledge`, knowledge of the parties' positions on European integration, with scores from 0 to 3.
* `gender`, `"female"` or `"male"`.

The model fit to the data includes an interaction between `Europe` and `political.knowledge`; the other predictors enter the model additively:
```{r BEPS-model}
library("nnet")
m.beps <- multinom(vote ~ age + gender + economic.cond.national +
                       economic.cond.household + Blair + Hague + Kennedy +
                       Europe*political.knowledge, data=BEPS)

car::Anova(m.beps)
```
Most of the predictors, including the `Europe` $\times$ `political.knowledge` interaction, are associated with very small $p$-values; the `Anova()` function is from the **car** package [@FoxWeisberg:2019].

Here's an "effect plot", using the the **effects** package [@FoxWeisberg:2019] to visualize the `Europe` $\times$ `political.knowledge` interaction in a "stacked-area" graph:
```{r BEPS-plot, fig.width=9, fig.height=5}
plot(effects::Effect(c("Europe", "political.knowledge"), m.beps,
            xlevels=list(Europe=1:11, political.knowledge=0:3),
            fixed.predictors=list(given.values=c(gendermale=0.5))),
     lines=list(col=c("blue", "red", "orange")),
     axes=list(x=list(rug=FALSE), y=list(style="stacked")))
```

To cross-validate this multinomial-logit model we need an appropriate cost criterion. Neither of the criteria supplied by the **cv** package---`mse()`, which is appropriate for a numeric response, nor `BayesRule()`, which is appropriate for a binary response---will do. One possibility is to adapt Bayes rule to a polytomous response:
```{r BayesRuleMulti}
head(BEPS$vote)
yhat <- predict(m.beps, type="class")
head(yhat)

BayesRuleMulti <- function(y, yhat){
  mean(y != yhat)
}

BayesRuleMulti(BEPS$vote, yhat)
```
The `predict()` method for `"multinom"` models called with argument `type="class"` reports the Bayes-rule prediction for each case---that is, the response category with the highest predicted probability. Our `BayesRuleMulti()` function just reports the proportion of misclassified cases. The marginal proportions for the response categories are
```{r BEPS-response-distribution}
xtabs(~ vote, data=BEPS)/nrow(BEPS)
```
and so the marginal Bayes-rule prediction, that everyone will vote Labour, produces an error rate of $1 - 0.47213 = 0.52787$. The multinomial-logit model appears to do substantially better than that, but does its performance hold up to cross-validation?

We check first whether the default `cv()` method works "out-of-the-box" for the `"multinom"` model:
```{r BEPS-test-default, error=TRUE}
cv(m.beps, seed=3465, criterion=BayesRuleMulti)
```
The default method of `getResponse()` (a function supplied by the **cv** package---see `?getResponse`) fails for a `"multinom"` object. A straightforward solution is to supply a `getResponse.multinom()` method that returns the factor response [using the `get_response()` function from the **insight** package, @LudeckeWaggonerMakowski:2019],
```{r getRespons.multinom}
getResponse.multinom <- function(model, ...) {
  insight::get_response(model)
}

head(getResponse(m.beps))
```
and to try again:
```{r BEPS-test-default-2, error=TRUE}
cv(m.beps, seed=3465, criterion=BayesRuleMulti)
```
A `traceback()` (not shown) reveals that the problem is that the default method of `cv()` calls the `"multinom"` method for `predict()` with the argument `type="response"`, when the correct argument should be `type="class"`.  We therefore must write a "`multinom`" method for `cv()`, but that proves to be very simple:
```{r cv.nultinom}
cv.multinom <- function (model, data, criterion=BayesRuleMulti, k, reps,
                         seed, ...){
  NextMethod(type="class", criterion=criterion)
}
```
That is, we simply call the default `cv()` method with the `type` argument properly set. In addition to supplying the correct `type` argument, our method sets the default `criterion` for the `cv.multinom()` method to `BayesRuleMulti`. 

Then:
```{r BEPS-cv}
m.beps <- update(m.beps, trace=FALSE)
cv(m.beps, seed=3465)
```
Prior to invoking `cv`, we called `update()` with `trace=FALSE` to suppress the iteration history reported by default by `multinom()`---it would be tedious to see the iteration history for each fold. The cross-validated polytomous Bayes-rule criterion confirms that the fitted model does substantially better than the marginal Bayes-rule prediction that everyone votes for Labour.


## Adding a model-selection procedure

The `selectStepAIC()` function supplied by the **cv** package, which is based on the `stepAIC()` function from the **nnet** package [@VenablesRipley:2002] for stepwise model selection, is suitable for the `procedure` argument of `cvSelect()`. The use of `selectStepAIC()` is illustrated in the principal vignette for the package. We'll employ `selectStepAIC()` as a "template" for writing a CV model-selection procedure. To see the code for this function, type `cv::selectStepAIC` at the R command prompt or examine the sources for the **cv** package at https://github.com/gmonette/cv.

Another approach to model selection is all-subsets regression. The `regsubsets()` function in the **leaps** package [@LumleyMiller:2020] implements an efficient algorithm for selecting the best-fitting linear least-squares regressions for subsets of predictors of all sizes, from 1 through the maximum number of candidate predictors. To illustrate its use, we employ the `swiss` data frame supplied by the **leaps** package:
```{r swiss}
library("leaps")
head(swiss)
nrow(swiss)
```
The data set includes the following variables, for each of 47 French-speaking Swiss provinces circa 1888:

* `Fertility`: A standardized fertility measure.
* `Agriculture`: The percentage of the male population engaged in agriculture.
* `Examination`: The percentage of draftees into the Swiss army receiving the highest grade on an examination.
* `Education`: The percentage of draftees with more than a primary-school education.
* `Catholic`: The percentage of the population who were Catholic.
* `Infant.Mortality`: The infant-mortality rate, expressed as the percentage of live births surviving less than a year.

Following @LumleyMiller:2020, we'll treat `Fertility` as the response and the other variables as predictors in a linear least-squares regression:'
```{r swiss-lm}
m.swiss <- lm(Fertility ~ ., data=swiss)
summary(m.swiss)

mse(swiss$Fertility, fitted(m.swiss))

cv(m.swiss, seed=8433)
```
Thus, the MSE for the model fit to the complete data is considerably smaller than the CV estimate of the MSE. Can we do better by selecting a subset of the predictors, taking account of the additional uncertainty induced by model selection?

First, let's apply best-subset selection to the complete data set:
```{r subset-selection}
#| fig.cap = "Selecting the best model of each size."
swiss.sub <- regsubsets(Fertility ~ ., data=swiss)
summary(swiss.sub)
(bics <- summary(swiss.sub)$bic)
which.min(bics)
car::subsets(swiss.sub, legend="topright")
```
The graph, produced by the `subsets()` function in the **car** package, shows that the model with the smallest BIC is the best model with 4 predictors, including `Agriculture`, `Education`, `Catholic`, and `Infant.Mortality`:
```{r best-model}
m.best <- update(m.swiss, . ~ . - Examination)
summary(m.best)

mse(swiss$Fertility, fitted(m.best))

cv(m.best, seed=8433) # use same folds as before
```
The MSE for the selected model is (of course) slightly higher than for the full model fit previously, but the cross-validated MSE is a bit lower---but, as we explain in the main vignette, it isn't kosher to select and cross-validate a model on the same data.

Here's a function named `selectSubsets()`, meant to be used with `cvSelect()`, so we can cross-validate the model-selection process:
```{r selectSubsets}
selectSubsets <- function(data=insight::get_data(model), 
                          model,
                          indices,
                          criterion=mse,
                          save.coef=TRUE, ...){
  if (inherits(model, "lm", which=TRUE) != 1)
    stop("selectSubsets is appropriate only for 'lm' models")
  y <- getResponse(model)
  formula <- formula(model)
  X <- model.matrix(model)

  if (missing(indices)) {
    # select best model from the full data by BIC
    sel <- leaps::regsubsets(formula, data=data, ...)
    bics <- summary(sel)$bic
    best <- coef(sel, 1:length(bics))[[which.min(bics)]]
    x.names <- names(best)
    m.best <- lm(y ~ X[, x.names])
    fit.all <- predict(m.best, newdata=data)
    return(criterion(y, fit.all))
  }

  # select the best model omitting the i-th fold (indices)
  sel.i <- leaps::regsubsets(formula, data[-indices, ], ...)
  bics.i <- summary(sel.i)$bic
  best.i <- coef(sel.i, 1:length(bics.i))[[which.min(bics.i)]]
  x.names.i <- names(best.i)
  m.best.i <- lm(y[-indices] ~ X[-indices, x.names.i] - 1)
              # predict() doesn't work here:
  fit.all.i <- as.vector(X[, x.names.i] %*% coef(m.best.i))
  fit.i <- fit.all.i[indices]
  list(criterion=c(criterion(y[indices], fit.i),
                   criterion(y, fit.all.i)),
       if (save.coef){
         coefs <- coef(m.best.i)
         nms <- names(coefs)
         # fix coefficient names
         nms <- sub("X\\[-indices, x.names.i\\]", "", nms)
         names(coefs) <- nms
         coefs
       }  else {
         NULL
       }
  )
}
```
A slightly tricky point is that because of scoping issues, `predict()` doesn't work with the model fit omitting the $i$th fold, and so the fitted values are computed directly as $\mathbf{X} \mathbf{b}_{-\mathbf{i}}$, where $\mathbf{X}$ is the model-matrix for all of the cases, and $\mathbf{b}_{-\mathbf{i}}$ is the vector of least-squares coefficients for the selected model with the $i$th fold omitted. Additionally, the command `lm(y[-indices] ~ X[-indices, x.names.i] - 1)`, which is the selected model with the $i$th fold deleted, produces awkward coefficient names like `"X[-indices, Agriculture]"`; the command `nms <- sub("X\\[-indices, x.names.i\\]", "", nms)` fixes these awkward names.

Applying `selectSubsets()` to the full data produces the full-data cross-validated MSE (which we obtained previously):
```{r test-selectSubsets}
selectSubsets(model=m.swiss)
```
Similarly, applying the function to an imaginary "fold" of the first 5 cases returns the MSE for the cases in the fold, the MSE for all of the cases based on the model selected and fit to the cases omitting the fold, and the coefficients of the selected model, which contains 3 predictors (and the intercept):
```{r test-selectSubsets-fold}
selectSubsets(model=m.swiss, indices=1:5)
```

Then, using `selectSubsets()` in cross-validation, we get:
```{r cvSelect-selectSubsets}
(cv.swiss <- cvSelect(selectSubsets, model=m.swiss,
                      data=swiss, seed=8433)) # use same folds
```
Cross-validation shows that model selection exacts a penalty in MSE. Examining the models selected for the 10 folds reveals that there is some uncertainty in identifying the predictors in the "best" model, with `Agriculture` sometimes appearing and sometimes not:
```{r best-models-by-folds}
compareFolds(cv.swiss)
```

## References





