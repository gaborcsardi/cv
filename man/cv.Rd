% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cv-mixed-models.R, R/cv.R
\name{cv.merMod}
\alias{cv.merMod}
\alias{cv}
\alias{cv.default}
\alias{print.cv}
\alias{cv.lm}
\alias{cv.glm}
\title{Cross-Validate Regression Models}
\usage{
\method{cv}{merMod}(
  model,
  data = insight::get_data(model),
  criterion = mse,
  k = nrow(clusters),
  seed,
  ncores = 1,
  clusterVariables,
  ...
)

cv(model, data, criterion, k, seed, ...)

\method{cv}{default}(
  model,
  data = insight::get_data(model),
  criterion = mse,
  k = 10,
  seed,
  ncores = 1,
  method = NULL,
  ...
)

\method{print}{cv}(x, digits = getOption("digits"), ...)

\method{cv}{lm}(
  model,
  data = insight::get_data(model),
  criterion = mse,
  k = 10,
  seed,
  method = c("auto", "hatvalues", "Woodbury", "naive"),
  ncores = 1,
  ...
)

\method{cv}{glm}(
  model,
  data = insight::get_data(model),
  criterion = mse,
  k = 10,
  seed,
  method = c("exact", "hatvalues", "Woodbury"),
  ncores = 1,
  ...
)
}
\arguments{
\item{model}{a regression model object (see Details).}

\item{data}{data frame to which the model was fit (not usually necessary)}

\item{criterion}{cross-validation criterion function of form \code{f(y, yhat)}
where \code{y} is the observed values of the response and
\code{yhat} the predicted values; the default is \code{\link{mse}}
(the mean-squared error)}

\item{k}{perform k-fold cross-validation (default is \code{10}); \code{k}
may be a number or \code{"loo"} or \code{"n"} for n-fold (leave-one-out)
cross-validation.}

\item{seed}{for R's random number generator; optional, if not
supplied a random seed will be selected and saved; not needed
for n-fold cross-validation}

\item{ncores}{number of cores to use for parallel computations
(default is \code{1}, i.e., computations aren't done in parallel)}

\item{clusterVariables}{a character vector of names of the variables
defining clusters for a mixed model with nested random effects}

\item{...}{to match generic}

\item{method}{computational method to apply to a linear (i.e. \code{"lm"}) model
or to a generalized linear (i.e., \code{"glm"}) model. See Details for an explanation
of the available options.}

\item{x}{a \code{cv} object to be printed}

\item{digits}{significant digits for printing,
default taken from the \code{"digits"} option}
}
\value{
a \code{"cv"} object with the cross-validation criterion averaged across the folds,
the bias-adjusted averaged CV criterion,
the criterion applied to the model fit to the full data set,
and the initial value of R's RNG seed. Some methods return a
subset of these components and may add additional information.
}
\description{
A parallelized generic k-fold (including n-fold, i.e., leave-one-out)
cross-validation function, with a default method, and
specific methods for linear and generalized-linear models that can be much
more computationally efficient.
}
\details{
The default method uses \code{\link{update}()} to refit the model
to each fold, and should work if there are appropriate \code{update()}
and \code{\link{predict}()} methods and if the default method for \code{\link{getResponse}()}
works or if a \code{getResponse()} method is supplied.

The \code{"lm"} and \code{"glm"} methods can use much faster computational
algorithms, as selected by the \code{method} argument. The linear-model
method accommodates weighted linear models.

For both classes of models, for the leave-one-out (n-fold) case, fitted values
for the folds can be computed from the hat-values via
\code{method="hatvalues"} without refitting the model;
for GLMs, this method is approximate, for LMs it is exact.

Again for both classes of models, when more than one case is omitted
in each fold, fitted values may be obtained without refitting the
model by exploiting the Woodbury matrix identity via \code{method="Woodbury"}.
As for hatvalues, this method is exact for LMs and approximate for
GLMs.

The default for linear models is \code{method="auto"},
which is equivalent to \code{method="hatvalues"} for n-fold cross-validation
and \code{method="Woodbury"} otherwise; \code{method="naive"} refits
the model via \code{update()} and is generally much slower. The
default for generalized linear models is \code{method="exact"},
which employs \code{update()}.
}
\section{Methods (by class)}{
\itemize{
\item \code{cv(merMod)}: merMod method

\item \code{cv(default)}: \code{default} method

\item \code{cv(lm)}: \code{"lm"} method

\item \code{cv(glm)}: \code{"glm"} method

}}
\section{Methods (by generic)}{
\itemize{
\item \code{print(cv)}: \code{print()} method

}}
\examples{
data("Auto", package="ISLR2")
m.auto <- lm(mpg ~ horsepower, data=Auto)
cv(m.auto,  k="loo")
cv(m.auto, seed=1234)

data("Caravan", package="ISLR2")
m.caravan <- glm(Purchase ~ ., data=Caravan[1:2500, ], family=binomial)
cv(m.caravan, k=5, criterion=BayesRule, seed=123)
}
