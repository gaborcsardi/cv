---
title: "Cross-validation of regression models: mixed model examples"
author: "John Fox and Georges Monette"
date: "`r Sys.Date()`"
package: cv
output: 
  rmarkdown::html_vignette:
  fig_caption: yes
bibliography: ["cv.bib"]
csl: apa.csl
vignette: >
  %\VignetteIndexEntry{Cross-validation of regression models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  message = TRUE,
  warning = TRUE,
  fig.align = "center",
  fig.height = 6,
  fig.width = 7,
  fig.path = "fig/",
  dev = "png",
  comment = "#>" #,
  # eval = nzchar(Sys.getenv("REBUILD_VIGNETTES"))
)

# save some typing
knitr::set_alias(w = "fig.width",
                 h = "fig.height",
                 cap = "fig.cap")

# colorize text: use inline as `r colorize(text, color)`
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}


.opts <- options(digits = 5)
```

```{r packages, include=FALSE}
library(magrittr)
library(bibtex)
library(kableExtra)
library(latex2exp)
library(cv)
library(car)
library(lattice)
library(latticeExtra)
layer <- latticeExtra::layer  # to mask ggplot2::layer
library(lme4)
library(nlme)
knitr::opts_chunk$set(comment='    ')
```
The following artificial data set exemplifies a difference between cluster-based
and individual-based cross-validation in hierarchical models.

It also shows how, in contrast with ordinary least-squares
models, the variance at different levels of a hierarchical
model need not necessarily become smaller as the model becomes larger.

In this example, the simplest random intercept model results in a between-cluster variance
that is much smaller than that of a model that also includes $x$ as a predictor.
Consequently, different methods of assessing model fit may result in
different best models, underscoring the importance of assessing hierarchical models
at different levels.

Adjusting the parameters generating this data can create an example 
that illustrates problems of non-convergence with hierarchical models.
Fitted models with relatively small between-cluster variance, even simple
random intercept models, are prone to non-convergence due to the 
nature of optimizing algorithms used in `lme` and `lmer`.

In some contexts, non-convergence,
can be viewed as a diagnostic of a poor model. However, with a simple 
random intercept model where it may indicate a small between-cluster
variance, non-convergence is an annoyance since it would be statistically correct to
report that the data are consistent with a zero between-cluster population variance,
i.e. that observed between-cluster variance in the data can be explained by 
the between-individual variability within clusters.  This leads to perfectly
valid statistical inferences. Although the calculation of accurate p-values for hypotheses concerning
random effects may be complicated technically
by the fact that the estimated variance is at the boundary of the parameter space,
the estimated variance-covariance matrix of fixed-effect parameters will generally
be non-singular and may be used for valid Wald tests of linear hypotheses 
of fixed-effect parameters.

Using cross-validation based on fitting random subsets compounds the problem
since subsets on which there is non-convergence cause the cross-validation to
fail unless non-converging subsets are omitted, which may bias results since non-convergence
is likely to be related to the estimated between-cluster variances.

By varying the parameter for between-cluster variance, we can use the approach
below to generate data sets with varying propensities to produce
non-convergence in order to explore the consequences of non-convergence and to 
compare the performance of different optimizers. **to do**

At present, the results below are based on a data set that does not
produce non-convergence and are limited to comparisons obtained
by varying three factors:

- Three models: random intercept only, adding x as a predictor, and also adding the group means of x as a predictor,
- The method used for fitting: `lmer` in `lme4` or `lme` in `nlme`,
- Cross-validation resampling individual cases or resampling clusters.

Note that since the independence of clusters is a reasonable assumption
while the independence of individual observations is not, 
cluster-based cross-validation has a stronger theoretical justification. 

We start by generating and plotting the data.

```{r data}
{
  # Generate data set
  
  # Parameters
  
  set.seed(123)
  Nb <- 10       # number of groups
  Nw <- 5        # number of individuals within groups
  Bb <- 0        # between group regression coefficient on group mean
  SDre <- 2.0    # between group SD of random level relative to group mean of X
  
  SDwithin <- 0.5    # within group SD
  
  Bw <- 1       # within group effect of X
  
  dd <- data.frame(
    group = factor(rep(1:Nb, each = Nw)),
    x = 20 + seq(1, Nb*Nw)
  ) %>% 
    within(
      {
        xm  <- ave(x, group, FUN = mean) # withing group mean
        y <- 10 +
          Bb * xm +                  # contextual effect
          Bw * (x - xm) +            # within-group effect
          SDre * rnorm(Nb)[group] +  # random level by group
          SDwithin * rnorm(Nb*Nw)    # random error within groups
      }
    )
}
```


```{r plot1}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Hierarchical data set with 10 clusters"
pl <- xyplot(y ~ x, dd, groups = group,
             ylim = c(0,20),
             par.settings = list(superpose.symbol = list(pch = 16, cex = 1))) + 
    latticeExtra::layer(panel.ellipse(...,center.cex =0))
pl
pl <- update(pl, par.settings = list(superpose.symbol = list(pch = 1, cex = .7)))
```

The Y variable could represent scores on a programming aptitude test of 10 students
of varying ages, each tested
annually during a 5-year training program.  The X variable is the age
at which each test occurred. 

We next obtain six fits with three models and two fitting functions.
```{r}
# random intercept models
 
fit0 <- lmer(y ~ 1 + (1|group), dd)
fit0l <- lme(y ~ 1, dd, random = ~ 1|group)

# effect of x and random intercept

fit1 <- lmer(y ~ x + (1|group), dd)
fit1l <- lme(y ~ x, dd, random = ~ 1|group)

# effect of x, mean of x and random intercept

fit2 <- lmer(y ~ x + xm + (1|group), dd)
fit2l <- lme(y ~ x + xm, dd, random = ~1|group)

modlist <- list(fit0, fit0l, fit1, fit1l, fit2, fit2l)
names(modlist) <- sapply(modlist, formula)
```
We can compare estimated fixed-effect coefficients.

```{r, message=FALSE,warning=FALSE}
tbl <- do.call(compareCoefs, modlist) 
tbl[] <- round(tbl,4)
tbl[is.na(tbl)] <- ''
kbl(tbl, align = 'r', caption = "Comparison of estimated fixed effects")
```

With this data set, the two functions `lme` and `lmer` converged for all models and produced consistent results. 
The differences between these two functions, and differences that depend on the optimizer used
within each function, would manifest themselves if the data were generated with a smaller
between-cluster variance. 

We visualize predicted values using population predictions with the fixed effects model only 
and using BLUPs for random effects.  Since `lme` and `lmer` produced almost identical results, the
figures below are produced only with the `lmer` results.

```{r}
dd$fit_mod0__lmer.fe <- predict(fit0, re.form = ~ 0)
dd$fit_mod0__lmer.re <- predict(fit0)
dd$fit_mod0__lme.fe <- predict(fit0l, level = 0)
dd$fit_mod0__lme.re <- predict(fit0l, level = 1)

dd$fit_mod1__lmer.fe <- predict(fit1, re.form = ~ 0)
dd$fit_mod1__lmer.re <- predict(fit1)
dd$fit_mod1__lme.fe <- predict(fit1l, level = 0)
dd$fit_mod1__lme.re <- predict(fit1l, level = 1)

dd$fit_mod2__lmer.fe <- predict(fit2, re.form = ~ 0)
dd$fit_mod2__lmer.re <- predict(fit2)
dd$fit_mod2__lme.fe <- predict(fit2l, level = 0)
dd$fit_mod2__lme.re <- predict(fit2l, level = 1)

dl <- stats::reshape(dd, direction = 'long', sep = '.', timevar = 'effect', varying = grep('\\.', names(dd)))
dl$id <- 1:nrow(dl)
dl <- stats::reshape(dl, direction = 'long', sep = '__', timevar = 'method',  varying = grep('__', names(dl)))
dl$id <- 1:nrow(dl)
dl <- stats::reshape(dl, direction = 'long', sep = '_', timevar = 'modelcode',  varying = grep('_', names(dl)))

dl$model <- factor(c('~ 1', '~ 1 + x', '~ 1 + x + xm')[match(dl$modelcode, c('mod0','mod1','mod2'))])

```


```{r plot-fits-mod0}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Fitted values using lmer with random intercept model"

(pl +
  xyplot(fit ~ x, subset(dl, modelcode == 'mod0' & effect == 'fe' & method == 'lmer'), groups = group, type = 'l', lwd =2) + 
  xyplot(fit ~ x, subset(dl, modelcode == 'mod0' & effect == 're' & method == 'lmer'), groups = group, type = 'l', lwd =2, lty=3) 
) %>% update(
  key=list(
    space = 'right', 
    text = list(c('fixed effects only','fixed and random')),
    lines = list(lty=c(1,3))))

```
```{r plot-fits-mod1}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Fitted values using lmer with random intercept model and x as a predictor"
(pl +
  xyplot(fit ~ x, subset(dl, modelcode == 'mod1' & effect == 'fe' & method == 'lmer'), groups = group, type = 'l', lwd =2) + 
  xyplot(fit ~ x, subset(dl, modelcode == 'mod1' & effect == 're' & method == 'lmer'), groups = group, type = 'l', lwd =2, lty=3) 
) %>% update(
  ylim = c(-20,40),
  key=list(
    space = 'right', 
    text = list(c('fixed effects only','fixed and random')),
    lines = list(lty=c(1,3))))
```
```{r plot-fits-mod2}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Fitted values using lmer with random intercept model and x and the group mean of x as predictors"
(pl +
  xyplot(fit ~ x, subset(dl, modelcode == 'mod2' & effect == 'fe' & method == 'lmer'), groups = group, type = 'l', lwd =2) + 
  xyplot(fit ~ x, subset(dl, modelcode == 'mod2' & effect == 're' & method == 'lmer'), groups = group, type = 'l', lwd =2, lty=3) 
) %>% update(
  ylim = c(0,20),
  key=list(
    space = 'right', 
    text = list(c('fixed effects only','fixed and random')),
    lines = list(lty=c(1,3))))
```

### Cross-validation of models

```{r cross-validation}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Cross-validation using lmer with random intercept model and x and the group mean of x as predictors"

cvs_cases <- lapply(modlist, cv) %>% lapply(rbind) %>% do.call(rbind,.)
cvs_clusters <- lapply(modlist, cv, cluster = 'group') %>% lapply(rbind) %>% do.call(rbind,.)

dout <- as.data.frame( rbind( cvs_cases, cvs_clusters))
# nrow(dout)
dout$cv_method <- rep(c('cases','clusters'), each = nrow(dout)/2)
dout$model <- rep(names(modlist), 2)
# dout$model <- factor(rep(c('~ 1','~ x','~ x + xm'), nrow(dout)/3))
dout$method <- rep(c('lmer','lme'), nrow(dout)/2)
dout$CV <- as.numeric(as.character(dout$`CV crit`))
dout$model <- factor(sub(' \\+ .1 .*$','', dout$model))

dout <- dout[order(dout$model),]
head(dout)
xyplot(CV ~ model , subset(dout, method == 'lmer'), groups = cv_method,
       par.settings = list(superpose.line = list(lty=c(2,3),lwd = 3)),
       type = 'l',
       auto.key = T,
       scales = list(x = list(alternating = F), y = list(log=T)))

```

