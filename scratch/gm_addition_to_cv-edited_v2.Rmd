---
title: "Cross-validation of regression models: mixed model examples (edited v2)"
author: "John Fox and Georges Monette"
date: "`r Sys.Date()`"
package: cv
output:
  rmarkdown::html_vignette:
  fig_caption: yes
bibliography: ["cv.bib"]
csl: apa.csl
vignette: >
  %\VignetteIndexEntry{Cross-validation of regression models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  message = TRUE,
  warning = TRUE,
  fig.align = "center",
  fig.height = 6,
  fig.width = 7,
  fig.path = "fig/",
  dev = "png",
  comment = "#>" #,
  # eval = nzchar(Sys.getenv("REBUILD_VIGNETTES"))
)

# save some typing
knitr::set_alias(w = "fig.width",
                 h = "fig.height",
                 cap = "fig.cap")

# colorize text: use inline as `r colorize(text, color)`
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color,
      x)
  } else x
}

.opts <- options(digits = 5)

knitr::opts_chunk$set(comment="    ")
```

```{r packages, include=FALSE}
library(cv)
library(car)
library(lattice)
library(latticeExtra)
library(lme4)
```
The following artificial data set exemplifies a difference between cluster-based
and individual-based cross-validation in hierarchical models.

It also shows how, in contrast with ordinary least-squares
models, the variance at different levels of a hierarchical
model need not necessarily become smaller as the model becomes larger.

In this example, the simplest random intercept model results in a between-cluster variance
that is much smaller than that of a model that also includes $x$ as a predictor.
Consequently, different methods of assessing model fit may result in
different best models, underscoring the importance of assessing hierarchical models
at different levels.

Adjusting the parameters generating this data can be used to create examples
that illustrates problems of non-convergence with hierarchical models.
Fitted models with relatively small between-cluster variance, even those using simple
random intercept models, are prone to non-convergence due to the
nature of optimizing algorithms used in `lme` and `lmer`.

In some contexts, non-convergence,
can be viewed as a diagnostic of a poor model. However, with a simple
random intercept model where it may indicate a small between-cluster
variance, non-convergence is an annoyance because it would be statistically correct to
report that the data are consistent with a zero between-cluster population variance,
i.e. that observed between-cluster variance in the data can be explained by
the between-individual variability within clusters.  This leads to
valid statistical inferences. Although the calculation of accurate p-values for hypotheses concerning
random effects may be complicated
by the fact that the estimated variance is at the boundary of the parameter space,
the estimated variance-covariance matrix of fixed-effect parameters will generally
be non-singular and may be used for valid Wald tests of linear hypotheses
of fixed-effect parameters.

Using cross-validation based on fitting random subsets compounds the problem
since subsets for which there is non-convergence cause the cross-validation to
fail unless non-converging subsets are omitted, which may bias results since non-convergence
is likely to be related to the estimated between-cluster variances.

Varying the parameter for between-cluster variance (`SDre` in the code below),
will generate data sets with varying propensities to produce
non-convergence. These data sets can be used to explore the consequences of non-convergence and to
compare the performance of different optimizers.

At present, the results below are based on a data set that does not
produce non-convergence and are limited to comparisons obtained
by varying three factors:

- Three models: random intercept only, adding x as a predictor, and also adding the group means of x as a predictor,
- The method used for fitting: `lmer` in `lme4` or `lme` in `nlme`,
- Cross-validation resampling individual cases or resampling clusters.

Since the independence of clusters may be a reasonable assumption, as is the
the independence of individual observations from the same cluster *conditional*
on clusters, each form of resampling addresses the
suitability of models
to answer different questions.
Results based on individual resampling generalize to new samples from the same
clusters. Resampling clusters generalizes to new samples of clusters and
cases.

We start by generating and plotting the data.

```{r data}
# Generate data set

# Parameters:
set.seed(9693) 
Nb <- 100    # number of groups
Nw <- 5      # number of individuals within groups
Bb <- 0      # between-group regression coefficient on group mean
SDre <- 2.0  # between-group SD of random level relative to group mean of x
SDwithin <- 0.5  # within group SD
Bw <- 1     # within group effect of x
Ay <- 10    # intercept for response
Ax <- 20    # starting level of x
Nx <- Nw*10 # number of distinct x values

Data <- data.frame(
  group = factor(rep(1:Nb, each=Nw)),
  x = Ax + rep(1:Nx, length.out = Nw*Nb)
) |>
  within(
    {
      xm  <- ave(x, group, FUN = mean) # within-group mean
      y <- Ay +
        Bb * xm +                  # contextual effect
        Bw * (x - xm) +            # within-group effect
        SDre * rnorm(Nb)[group] +  # random level by group
        SDwithin * rnorm(Nb*Nw)    # random error within groups
    }
  )
```


```{r plot1}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Hierarchical data set, showing first 10 of 100 clusters."
plot <- xyplot(y ~ x, data=Data[1:Nx, ], group=group,
               ylim=c(4, 16),
               par.settings=list(superpose.symbol=list(pch=1, cex=0.7))) +
    layer(panel.ellipse(..., center.cex=0))
plot
```

The Y variable could represent scores on a programming aptitude test of 10 students
of varying ages, each tested
annually during a 5-year training program.  The X variable is the age
at which each test occurred.

We next obtain six fits with three models and two fitting functions.
```{r}
# random intercept only:
fit0 <- lmer(y ~ 1 + (1 | group), Data)

# effect of x and random intercept:
fit1 <- lmer(y ~ x + (1 | group), Data)

# effect of x, mean of x, and random intercept:
fit2 <- lmer(y ~ x + xm + (1 | group), Data)
        # equivalent to y ~ I(x - xm) + xm + (1 | group)

# model generating the data (where Bb = 0)
fit3 <- lmer(y ~ I(x - xm) + (1 | group), Data)
```
We can compare estimated fixed-effect coefficients.

```{r, message=FALSE,warning=FALSE}
compareCoefs(fit0,  fit1, fit2, fit3)
```

With this data set, the two functions `lme` and `lmer` converged for all models and produced consistent results.
The differences between these two functions, and differences that depend on the optimizer used
within each function, would manifest themselves if the data were generated with a smaller
between-cluster variance.

We visualize predicted values using population predictions with the fixed effects model only
and using BLUPs for random effects.  Since `lme` and `lmer` produced almost identical results, the
figures below are produced only with the `lmer` results.

```{r}
Data <- within(Data, {
  fit_mod0.fe <- predict(fit0, re.form = ~ 0)
  fit_mod0.re <- predict(fit0)
  fit_mod1.fe <- predict(fit1, re.form = ~ 0)
  fit_mod1.re <- predict(fit1)
  fit_mod2.fe <- predict(fit2, re.form = ~ 0)
  fit_mod2.re <- predict(fit2)
  fit_mod3.fe <- predict(fit3, re.form = ~ 0)
  fit_mod3.re <- predict(fit3)
})

Data_long <- reshape(Data[1:Nx, ], direction = "long", sep = ".", 
              timevar = "effect", varying = grep("\\.", names(Data[1:Nx, ])))
Data_long$id <- 1:nrow(Data_long)
Data_long <- reshape(Data_long, direction = "long", sep = "_", 
              timevar = "modelcode",  varying = grep("_", names(Data_long)))

Data_long$model <- factor(
  c("~ 1", "~ 1 + x", "~ 1 + x + xm", "~ 1 + I(x - xm)")
  [match(Data_long$modelcode, c("mod0", "mod1", "mod2", "mod3"))]
)
```


```{r plot-fits-mod0}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Fitted values from random intercept model."
(plot +
  xyplot(fit ~ x, subset(Data_long, modelcode == "mod0" & effect == "fe"),
         groups=group, type="l", lwd=2) +
  xyplot(fit ~ x, subset(Data_long, modelcode == "mod0" &  effect == "re"),
         groups=group, type="l", lwd=2, lty=3)
) |> update(
  main="Model: y ~ 1 + (1 | group)",
  key=list(
    corner=c(0.05, 0.05),
    text=list(c("fixed effects only","fixed and random")),
    lines=list(lty=c(1, 3))))
```
```{r plot-fits-mod1}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Fitted values from model with random intercept and x as a predictor."
(plot +
  xyplot(fit ~ x, subset(Data_long, modelcode == "mod1" & effect == "fe"),
         groups=group, type="l", lwd=2) +
  xyplot(fit ~ x, subset(Data_long, modelcode == "mod1" & effect == "re"),
         groups=group, type="l", lwd=2, lty=3)
) |> update(
  main="Model: y ~ 1 + x + (1 | group)",
  ylim=c(-15, 35),
  key=list(
    corner=c(0.95, 0.05),
    text=list(c("fixed effects only","fixed and random")),
    lines=list(lty=c(1, 3))))
```
```{r plot-fits-mod2}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Fitted values from model with random intercept, x, and the group mean of x as predictors."
(plot +
  xyplot(fit ~ x, subset(Data_long, modelcode == "mod2" & effect == "fe"),
         groups=group, type="l", lwd=2) +
  xyplot(fit ~ x, subset(Data_long, modelcode == "mod2" & effect == "re"),
         groups=group, type="l", lwd=2, lty=3)
) |> update(
  main="Model: y ~ 1 + x + xm + (1 | group)",
  ylim=c(4, 16),
  key=list(
    corner=c(0.05, 0.05),
    text=list(c("fixed effects only","fixed and random")),
    lines=list(lty=c(1, 3))))
```

```{r plot-fits-mod3}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Fitted values from the estimated model generating the data."
(plot +
  xyplot(fit ~ x, subset(Data_long, modelcode == "mod3" & effect == "fe"),
         groups=group, type="l", lwd=2) +
  xyplot(fit ~ x, subset(Data_long, modelcode == "mod3" & effect == "re"),
         groups=group, type="l", lwd=2, lty=3)
) |> update(
  main="Model: y ~ 1 + I(x - xm) + (1 | group)",
  ylim=c(4, 16),
  key=list(
    corner=c(0.05, 0.05),
    text=list(c("fixed effects only","fixed and random")),
    lines=list(lty=c(1, 3))))
```

### Cross-validation of models

We carry out cross-validation resampling individuals within clusters, based on
fixed and predicted random effects, and resampling clusters, based on
fixed effects only.
In order to reduce between-model variability in comparisons of models,
the `cv` method applied to the list of models created by the `models` function,
performs cross-validation with the same training and validation sets
for each model in order to reduce the between-model variability in the cross-validation
criterion that stems from the random variation in training set selection.

```{r cross-validation-clusters}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Cross-validation comparing random intercept models with varying fixed effects and with cluster-based resampling."
modlist <- models("~ 1"=fit0, "~ 1 + x"=fit1, 
                  "~ 1 + x + xm"=fit2, "~ 1 + I(x - xm)"=fit3)
cvs_clusters <- cv(modlist, data=Data, cluster="group", k=10, seed=6449)
plot(cvs_clusters, main="Model Comparison, Cluster-Based CV")
```
```{r cross-validation-cases}
#| out.width = "100%",
#| fig.height = 6,
#| fig.cap = "Cross-validation comparing random intercept models with varying fixed effects and with case-based resampling."
cvs_cases <- cv(modlist, data=Data, seed=9693)
plot(cvs_cases, main="Model Comparison, Case-Based CV")
```

With this example, `lme` and `lmer` give similar results, but resampling cases versus resampling clusters provides
very different comparisons of models.  The model with X alone, without the contextual mean of X, is assessed as
fitting very poorly sampling clusters and, relatively, much better resampling individual cases.  This illustrates the
need to perform cross-validation at different levels.
